---
title: "Pr√°ctica 2 ‚Äî Feature Engineering simple + Modelo base"
date: 2025-08-16
---

# Pr√°ctica 2 ‚Äî Feature Engineering simple + Modelo base

## Contexto
En esta pr√°ctica trabajamos sobre el dataset del Titanic para aplicar t√©cnicas de **Feature Engineering** y entrenar un **modelo base de clasificaci√≥n**.  

Adem√°s, exploramos los componentes principales de Scikit-learn para clasificaci√≥n y validaci√≥n de modelos.

> **Nota**: Trabajamos con el dataset **Titanic** de Kaggle, que contiene informaci√≥n de pasajeros y si sobrevivieron o no.  

## Objetivos
- Comprender el uso de `LogisticRegression`, `DummyClassifier`, `train_test_split` y m√©tricas de evaluaci√≥n en Scikit-learn.  
- Crear nuevas variables a partir de las originales para mejorar el rendimiento del modelo.  
- Entrenar un modelo baseline y compararlo con una regresi√≥n log√≠stica.  

## Actividades (con tiempos estimados)

| Actividad                              | Tiempo | Resultado esperado                            |
|----------------------------------------|:------:|-----------------------------------------------|
| Investigaci√≥n de Scikit-learn          | 10m    | Conocer componentes clave                     |
| Preprocesamiento y Feature Engineering | 15m    | Dataset enriquecido y limpio                  |
| Entrenamiento baseline y LogReg        | 20m    | Modelos entrenados y m√©tricas obtenidas       |
| An√°lisis de resultados                 | 15m    | Reflexiones sobre errores y mejoras           |

## Desarrollo

### 1. Investigaci√≥n inicial
- **LogisticRegression**: Modelo lineal para clasificaci√≥n binaria/multiclase. Importante ajustar `solver`, `C`, `max_iter`. `liblinear` funciona bien para datasets peque√±os.  
- **DummyClassifier**: Proporciona un baseline simple (ej. siempre predecir la clase m√°s frecuente). Sirve para validar si un modelo ‚Äúaprende‚Äù algo m√°s all√° del azar.  
- **train_test_split**: `stratify` asegura proporciones similares entre train/test, `random_state` asegura reproducibilidad.  
- **M√©tricas de evaluaci√≥n**:  
  - *Accuracy*: proporci√≥n de aciertos.  
  - *Precision/Recall/F1*: m√°s informativas cuando las clases est√°n desbalanceadas.  
  - *Matriz de confusi√≥n*: muestra aciertos y errores por clase.  

### 2. Feature Engineering
- Imputaci√≥n de valores faltantes: `Embarked` (moda), `Fare` (mediana), `Age` (mediana por sexo y clase).  
- Nuevas variables:
  - `FamilySize = SibSp + Parch + 1`  
  - `IsAlone` (indicador binario)  
  - `Title` (extra√≠do de `Name`, agrupando t√≠tulos poco frecuentes en ‚ÄúRare‚Äù).  
- Resultado: **(891, 14) features** finales. (Ver c√≥digo en "Evidencias")

### 3. Modelado
- Baseline (DummyClassifier, estrategia "most_frequent").  
- Logistic Regression (`liblinear`, `max_iter=1000`).  
- Divisi√≥n: 80% train, 20% test, estratificada.  
- Resultados (Ver c√≥digo en "Evidencias"):
    - Baseline acc: 0.6145
    - LogReg acc : 0.8156


### 4. Evaluaci√≥n
**Classification report (LogReg):**

| Clase             | Precision | Recall | F1-score | Soporte |
|-------------------|-----------|--------|----------|---------|
| 0 (no sobrevivi√≥) |    0.82   |  0.89  |   0.86   |   110   |
| 1 (sobrevivi√≥)    |    0.80   |  0.70  |   0.74   |    69   |
| Accuracy          |           |        |   0.82   |   179   |
| macro avg         |    0.81   |  0.79  |   0.80   |   179   |
| weighted avg      |    0.81   |  0.82  |   0.81   |   179   |  

**Matriz de confusi√≥n (LogReg):**

|  Real          | Pred. No sobrevivi√≥ | Pred. Sobrevivi√≥ |
|----------------|---------------------|------------------|
|  No sobrevivi√≥ |         98          |         12       |
|  Sobrevivi√≥    |         21          |         48       |

## Evidencias
- [Notebook en Google Colab](https://colab.research.google.com/drive/1pCw_QCZsqQB8gcLmOhqY-XBz3gmItlxT?usp=sharing).
- **C√≥digo ejecutado para la parte 2. Feature Engineering:**
```python
df = train.copy()

# üö´ PASO 1: Manejar valores faltantes (imputaci√≥n)
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])  # Valor m√°s com√∫n
df['Fare'] = df['Fare'].fillna(df['Fare'].median())              # Mediana
df['Age'] = df['Age'].fillna(df.groupby(['Sex','Pclass'])['Age'].transform('median'))

# üÜï PASO 2: Crear nuevas features √∫tiles
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
df['IsAlone'] = (df['FamilySize'] == 1).astype(int)

df['Title'] = df['Name'].str.extract(',\s*([^\.]+)\.')
rare_titles = df['Title'].value_counts()[df['Title'].value_counts() < 10].index
df['Title'] = df['Title'].replace(rare_titles, 'Rare')

# üîÑ PASO 3: Preparar datos para el modelo
features = ['Pclass','Sex','Age','Fare','Embarked','FamilySize','IsAlone','Title','SibSp','Parch']
X = pd.get_dummies(df[features], drop_first=True)
y = df['Survived']

X.shape, y.shape
```

- **C√≥digo ejecutado para la parte 3.Modelado:**
```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

dummy = DummyClassifier(strategy='most_frequent', random_state=42)
dummy.fit(X_train, y_train)
baseline_pred = dummy.predict(X_test)

lr = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)
lr.fit(X_train, y_train)
pred = lr.predict(X_test)

print('Baseline acc:', accuracy_score(y_test, baseline_pred))
print('LogReg acc  :', accuracy_score(y_test, pred))

print('\nClassification report (LogReg):')
print(classification_report(y_test, pred))

print('\nConfusion matrix (LogReg):')
print(confusion_matrix(y_test, pred))
```

## Reflexi√≥n
- El modelo se equivoca m√°s al predecir que una persona sobrevivi√≥ cuando en realidad no lo hizo (21 errores vs 12 en la otra direcci√≥n).  
- Tiene mayor acierto en la clase "no sobrevivi√≥" (recall 0.89 vs 0.70 en "sobrevivi√≥").  
- Comparado con el baseline (61%), la Regresi√≥n Log√≠stica mejora significativamente (82%).  
- Error m√°s grave: en este contexto, predecir que alguien no sobrevivi√≥ cuando en realidad s√≠ lo hizo podr√≠a ser m√°s costoso desde el punto de vista humanitario.  
- Mejoras simples: incluir features como agrupar `Age` en categor√≠as (ni√±os, adultos, ancianos).  

## Referencias
- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)  
- [Kaggle ‚Äî Titanic Competition](https://www.kaggle.com/competitions/titanic/data)

