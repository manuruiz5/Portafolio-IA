
# üìù Pr√°ctica 6 ‚Äî Clustering y PCA - Mall Customer Segmentation

## Contexto

PROBLEMA DE SEGMENTACI√ìN: Los centros comerciales necesitan entender mejor a sus clientes para optimizar sus estrategias de marketing. El objetivo es segmentar clientes bas√°ndose en: - Personalizaci√≥n de campa√±as de marketing
- Ofertas espec√≠ficas por tipo de cliente - Optimizaci√≥n de inversi√≥n publicitaria - Comprensi√≥n de patrones de comportamiento de compra

DATASET DE TRABAJO: Utilizaremos el Mall Customer Segmentation Dataset que contiene informaci√≥n demogr√°fica y comportamental real de clientes de centros comerciales:

Fuente: Mall Customer Segmentation Dataset - Kaggle

Registros: ~200 clientes con informaci√≥n completa

Variables: CustomerID, Genre, Age, Annual Income, Spending Score

Ventaja: Datos limpios y listos para an√°lisis, perfectos para demostrar t√©cnicas de clustering

---

## üéØ Objetivos

- Identificar 3-5 segmentos de clientes distintos usando K-Means 
- Aplicar t√©cnicas de normalizaci√≥n (MinMax, Standard, Robust)
- Usar PCA para reducci√≥n de dimensionalidad y visualizaci√≥n 
- Comparar PCA con m√©todos de selecci√≥n de features 
- Interpretar resultados desde perspectiva de negocio

---

## Actividades 

| Actividad                                  | Resultado esperado                                                                 |
| ------------------------------------------ | ---------------------------------------------------------------------------------- |
| Setup Inicial                              | Librer√≠as **Pandas** y **NumPy** importadas y listas para an√°lisis.                |
| Carga del Dataset                          | Dataset Mall Customer Segmentation cargado correctamente en un DataFrame.          |
| Inspecci√≥n Inicial del Dataset             | Confirmaci√≥n de estructura (200 filas √ó 5 columnas) y primeras observaciones.      |
| An√°lisis de Tipos de Datos                 | Tipos verificados, sin valores nulos, estad√≠sticas descriptivas generadas.         |
| An√°lisis de Distribuci√≥n por G√©nero        | Distribuci√≥n balanceada (56% mujeres, 44% hombres).                                |
| Estad√≠sticas de Variables Clave            | Rangos y promedios de Age, Income y Spending Score identificados.                  |
| Detecci√≥n de Outliers                      | Outliers m√≠nimos detectados solo en ingresos (1%).                                 |
| Visualizaciones - Distribuciones           | Histogramas de Age, Income y Spending Score generados y analizados.                |
| Visualizaciones - Relaciones               | Scatter plots entre Age, Income y Spending Score; relaci√≥n clave Income‚ÄìScore.     |
| Matriz de Correlaci√≥n                      | Correlaciones bajas; Age ‚Üî Spending Score con relaci√≥n negativa moderada.          |
| An√°lisis Comparativo por G√©nero            | Diferencias observadas: mujeres con mayor Score, hombres con mayor Income y Age.   |
| S√≠ntesis de Insights                       | Principales hallazgos resumidos; Income y Score seleccionados como features clave. |
| Identificaci√≥n de Features para Clustering | Variables seleccionadas: Age, Income, Spending Score + g√©nero codificado.          |
| Codificaci√≥n de Variables Categ√≥ricas      | Variable `Genre` transformada con OneHotEncoder a Genre\_Female y Genre\_Male.     |
| Preparaci√≥n del Dataset Final              | Dataset completo con variables num√©ricas y categ√≥ricas listas para clustering.     |
| Verificaci√≥n de Calidad de Datos           | Dataset limpio, sin valores faltantes ni inconsistencias, listo para modelado.     |
| Decisi√≥n Final de Scaler                   | Se seleccion√≥ **MinMaxScaler** como mejor opci√≥n (Silhouette = 0.573).             |



## Desarrollo

### ‚öôÔ∏è 1. Setup Inicial  

En este primer paso realizamos la **configuraci√≥n inicial del entorno de trabajo** importando las bibliotecas esenciales: **Pandas** y **NumPy**.  

La motivaci√≥n es que antes de cualquier an√°lisis necesitamos herramientas que permitan:  

- **Pandas (`pd`)** ‚Üí trabajar con estructuras de datos como **DataFrames** y **Series**, que facilitan la carga, exploraci√≥n y manipulaci√≥n de datos tabulares.  
- **NumPy (`np`)** ‚Üí realizar **operaciones matem√°ticas y computacionales** sobre arreglos multidimensionales con alta eficiencia.  

Estas bibliotecas son la base de la mayor√≠a de los proyectos de ciencia de datos en Python.  

#### üîë Decisiones tomadas  

- Se import√≥ **Pandas** con el alias `pd` (convenci√≥n est√°ndar en la comunidad).  
- Se import√≥ **NumPy** con el alias `np`.  

#### üìä Resultado obtenido  

La ejecuci√≥n mostr√≥ en consola:  

> Iniciando an√°lisis de Mall Customer Segmentation Dataset  
> Pandas y NumPy cargados - listos para trabajar con datos  

Esto confirma que ambas librer√≠as se cargaron correctamente y podemos avanzar.

### üìÇ 2. Carga del Dataset  

En este paso realizamos la **carga del dataset Mall Customer Segmentation** desde un repositorio de GitHub.  

Para cualquier an√°lisis de datos, el primer paso es disponer del conjunto de datos en memoria para poder explorarlo, limpiarlo y modelarlo posteriormente.  

#### üîë Decisiones tomadas  

- Se utiliz√≥ la **URL directa al archivo CSV** en GitHub, lo cual asegura que los datos se descarguen siempre de una fuente confiable.  
- Se us√≥ la funci√≥n **`pd.read_csv()` de Pandas**, que permite leer archivos en formato CSV y convertirlos en un **DataFrame** (estructura tabular de filas y columnas).  
- Se almacen√≥ el dataset en la variable `df_customers`, que ser√° utilizada en los pasos siguientes.  

### üîé 3. Inspecci√≥n Inicial del Dataset  

En este paso realizamos una **exploraci√≥n preliminar del dataset** para conocer su tama√±o, estructura y primeras observaciones.  

Antes de cualquier transformaci√≥n o modelado, es fundamental entender **qu√© datos tenemos**, su cantidad, las variables disponibles y su formato.  

#### üîë Decisiones tomadas  

- Se imprimi√≥ la **forma del dataset** (`shape`) para verificar n√∫mero de filas y columnas.  
- Se listaron los **nombres de las columnas** para conocer las variables disponibles.  
- Se calcul√≥ el **uso de memoria** en kilobytes, √∫til para evaluar eficiencia en entornos de recursos limitados.  
- Se mostraron las **primeras 5 filas** con `.head()` para observar ejemplos concretos de los datos.  

#### üìä Resultado obtenido  

La ejecuci√≥n mostr√≥ en consola:  

**INFORMACI√ìN DEL DATASET:**

- Shape: 200 filas, 5 columnas
- Columnas: ['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']
- Memoria: 16.9 KB

**PRIMERAS 5 FILAS:**
CustomerID | Genre | Age | Annual Income (k$) | Spending Score (1-100) |
1          | Male  | 19  |        15          |           39           |
2          | Male  | 21  |        15          |           81           |
3          | Female| 20  |        16          |           6            |
4          | Female| 23  |        16          |           77           |
5          | Female| 31  |        17          |           40           |


#### üìà An√°lisis  

- El dataset contiene **200 clientes** y **5 variables**.  
- Las columnas disponibles son:  
     - **CustomerID** ‚Üí identificador √∫nico.  
     - **Genre** ‚Üí g√©nero (Male/Female).  
     - **Age** ‚Üí edad del cliente.  
      - **Annual Income (k$)** ‚Üí ingreso anual expresado en miles de d√≥lares.  
     - **Spending Score (1-100)** ‚Üí puntuaci√≥n de gasto asignada por el shopping.  
- El consumo de memoria es bajo (**16.9 KB**), por lo que no habr√° problemas de rendimiento.  
- La inspecci√≥n inicial confirma que los datos est√°n completos y correctamente estructurados para avanzar con la exploraci√≥n m√°s detallada.  

### üß© 4. An√°lisis de Tipos de Datos  

En este paso realizamos un **an√°lisis detallado de la estructura del dataset y de las estad√≠sticas b√°sicas de cada variable**.  

La motivaci√≥n es conocer qu√© tipo de datos tenemos, verificar que no haya valores nulos y observar las primeras m√©tricas descriptivas que nos permiten entender la distribuci√≥n de las variables.  

#### üîë Decisiones tomadas  

Se utiliz√≥ `df_customers.info()` para obtener:  

  - N√∫mero total de filas y columnas.  
  - Tipos de datos de cada variable (num√©rica o categ√≥rica).  
  - Cantidad de valores no nulos, √∫til para detectar datos faltantes.  

Se aplic√≥ `df_customers.describe()` para generar estad√≠sticas descriptivas de las columnas num√©ricas:  

  - **Media, desviaci√≥n est√°ndar, m√≠nimo, m√°ximo y percentiles (25%, 50% y 75%)**.  

Esta combinaci√≥n permite una primera validaci√≥n de la **calidad y consistencia del dataset**.  

#### üìä Resultado obtenido  

La ejecuci√≥n mostr√≥ en consola:  

**INFORMACI√ìN DETALLADA DE COLUMNAS:**

 #  | Column                |  Non-Null Count | Dtype |
--- | ----------------------|  -------------- | ----- |
 0  | CustomerID            |  200 non-null   | int64 |
 1  | Genre                 |  200 non-null   | object|
 2  | Age                   |  200 non-null   | int64 |
 3  | Annual Income (k$)    |  200 non-null   | int64 |
 4  | Spending Score (1-100)|  200 non-null   | int64 |

**ESTAD√çSTICAS DESCRIPTIVAS:**


|           | CustomerID | Age        | Annual Income (k\$) | Spending Score (1-100) |
| --------- | ---------- | ---------- | ------------------- | ---------------------- |
| **count** | 200.000000 | 200.000000 | 200.000000          | 200.000000             |
| **mean**  | 100.500000 | 38.850000  | 60.560000           | 50.200000              |
| **std**   | 57.879185  | 13.969007  | 26.264721           | 25.823522              |
| **min**   | 1.000000   | 18.000000  | 15.000000           | 1.000000               |
| **25%**   | 50.750000  | 28.750000  | 41.500000           | 34.750000              |
| **50%**   | 100.500000 | 36.000000  | 61.500000           | 50.000000              |
| **75%**   | 150.250000 | 49.000000  | 78.000000           | 73.000000              |
| **max**   | 200.000000 | 70.000000  | 137.000000          | 99.000000              |

#### üìà An√°lisis  

 **Tipos de datos**:  

  - 4 variables num√©ricas (`int64`).  
  - 1 variable categ√≥rica (`object`) ‚Üí **Genre**. 

**Valores nulos**: no se detectaron, ya que todas las columnas tienen 200 registros v√°lidos.  

**Distribuci√≥n de las variables num√©ricas**:  

  - **Age**: promedio de ~39 a√±os, con m√≠nimo 18 y m√°ximo 70.  
  - **Annual Income (k$)**: media de ~60.5, rango entre 15 y 137 ‚Üí muestra una amplia variaci√≥n de ingresos.  
  - **Spending Score (1-100)**: promedio cercano a 50, con distribuci√≥n bastante dispersa (1 a 99).  

**CustomerID** es solo un identificador, por lo que no aporta informaci√≥n relevante para el an√°lisis de segmentaci√≥n y podr√° ser descartado m√°s adelante.  

El dataset es **limpio, completo y con tipos de datos adecuados** para aplicar t√©cnicas de segmentaci√≥n.  

### üîç 5. An√°lisis de Distribuci√≥n por G√©nero

En este paso realizamos un an√°lisis exploratorio de la variable categ√≥rica **Genre** para observar la distribuci√≥n de clientes por sexo dentro del dataset.

El objetivo es verificar si existe un balance o sesgo en la representaci√≥n de g√©neros, lo que podr√≠a tener implicaciones en los futuros procesos de segmentaci√≥n.

#### üîë Decisiones tomadas

- Se utiliz√≥ `df_customers['Genre'].value_counts()` para obtener la frecuencia absoluta de cada categor√≠a.  
- Se calcularon los porcentajes relativos dividiendo cada frecuencia entre el total de registros (200).  
- Se verific√≥ que no hubiera categor√≠as adicionales o valores at√≠picos.  

#### üìä Resultado obtenido

**DISTRIBUCI√ìN POR G√âNERO (frecuencia absoluta):**

| G√©nero | Cantidad |
|--------|----------|
| Female | 112      |
| Male   | 88       |

**DISTRIBUCI√ìN POR G√âNERO (porcentaje):**

| G√©nero | Porcentaje |
|--------|------------|
| Female | 56.0%      |
| Male   | 44.0%      |

#### üìà An√°lisis

El dataset presenta una distribuci√≥n relativamente balanceada entre g√©neros, aunque con una ligera mayor√≠a de mujeres (56%).  

Esta proporci√≥n no muestra un sesgo cr√≠tico, por lo que el dataset se puede utilizar para segmentaciones sin necesidad de aplicar t√©cnicas de balanceo.  

La variable **Genre** ser√° √∫til para analizar diferencias de comportamiento de consumo entre hombres y mujeres en pasos posteriores.  

La muestra presenta una ligera mayor√≠a femenina, pero ambos g√©neros est√°n suficientemente representados para un an√°lisis confiable.

### üí° 6. Estad√≠sticas de Variables Clave

En este paso se analizaron las **variables num√©ricas de segmentaci√≥n** incluidas en el dataset: 

- **Age** (Edad)  
- **Annual Income (k$)** (Ingresos anuales en miles de d√≥lares)  
- **Spending Score (1-100)** (√çndice de gasto en una escala de 1 a 100)  

El objetivo es conocer la distribuci√≥n general, los rangos y la dispersi√≥n de cada variable para identificar patrones iniciales antes de aplicar t√©cnicas de segmentaci√≥n.

#### üìä Resultado obtenido

**Estad√≠sticas descriptivas principales:**

| Variable                | Media | Desv. Est. | Min | Q1   | Mediana | Q3   | Max |
|--------------------------|-------|------------|-----|------|---------|------|-----|
| Age                      | 38.85 | 13.97      | 18  | 28.75| 36.00   | 49.00| 70  |
| Annual Income (k$)       | 60.56 | 26.26      | 15  | 41.50| 61.50   | 78.00| 137 |
| Spending Score (1-100)   | 50.20 | 25.82      | 1   | 34.75| 50.00   | 73.00| 99  |

**Rangos observados:**

- **Age**: 18 ‚Äì 70 (promedio: 38.9)  
- **Annual Income (k$)**: 15 ‚Äì 137 (promedio: 60.6)  
- **Spending Score (1-100)**: 1 ‚Äì 99 (promedio: 50.2)  

#### üìà An√°lisis

- **Edad (Age):** el rango va de 18 a 70 a√±os, con un promedio cercano a 39 a√±os. La dispersi√≥n es moderada, lo que indica diversidad etaria en la muestra.  
- **Ingresos Anuales (Annual Income):** oscilan entre 15k y 137k d√≥lares, con un promedio de 60.6k. La amplitud del rango refleja distintos perfiles socioecon√≥micos dentro del dataset.  
- **Spending Score:** var√≠a entre 1 y 99, con un promedio de 50.2 y una dispersi√≥n considerable, lo que sugiere diferencias marcadas en los h√°bitos de consumo.  

Las tres variables presentan suficiente variabilidad y amplitud de rangos, lo que resulta favorable para realizar segmentaciones efectivas en etapas posteriores del an√°lisis.

### ‚öôÔ∏è 7. Detecci√≥n de Outliers

En este paso se realiz√≥ la **detecci√≥n de valores at√≠picos (outliers)** en las variables num√©ricas clave utilizando el **m√©todo del IQR (Interquartile Range)**.  

El objetivo es identificar observaciones que se encuentren significativamente alejadas del rango t√≠pico de la variable, ya que estos outliers podr√≠an influir en los an√°lisis de segmentaci√≥n o modelos posteriores.

#### üìä Resultado obtenido

| Variable                 | # Outliers | % Outliers | L√≠mites normales    |
|--------------------------|------------|------------|---------------------|
| Age                      | 0          | 0.0%       | -1.6 ‚Äì 79.4         |
| Annual Income (k$)       | 2          | 1.0%       | -13.2 ‚Äì 132.8       |
| Spending Score (1-100)   | 0          | 0.0%       | -22.6 ‚Äì 130.4       |

#### üìà An√°lisis

- **Edad (Age):** no se detectaron outliers, lo que indica que la distribuci√≥n es relativamente uniforme y sin valores extremos.  
- **Ingresos Anuales (Annual Income):** se encontraron 2 outliers (1%), lo que representa una proporci√≥n muy baja respecto al total del dataset. Estos valores extremos podr√≠an analizarse posteriormente para decidir si se mantienen o se ajustan.  
- **Spending Score:** tampoco se detectaron outliers, sugiriendo que la mayor√≠a de los clientes se encuentra dentro de un rango esperado de h√°bitos de gasto.  

El dataset presenta muy pocos valores at√≠picos, lo que permite utilizarlo con confianza para an√°lisis de segmentaci√≥n sin necesidad de un tratamiento extensivo de outliers.

### üìä 8. Visualizaciones - Distribuciones

En este paso se generaron **visualizaciones gr√°ficas** para analizar la distribuci√≥n de las principales variables num√©ricas del dataset:  

- **Age (Edad)**  
- **Annual Income (k$)** (Ingresos anuales en miles de d√≥lares)  
- **Spending Score (1-100)** (√çndice de gasto en una escala de 1 a 100)  

El objetivo fue obtener una representaci√≥n visual que complemente las estad√≠sticas descriptivas y facilite la identificaci√≥n de patrones, concentraciones y posibles valores extremos.

#### ‚öôÔ∏è Decisiones tomadas

Se completaron los espacios en blanco con:  

  - `matplotlib.pyplot` como biblioteca principal para **gr√°ficos b√°sicos**.  
  - `seaborn` como biblioteca de **visualizaci√≥n estad√≠stica** con estilos y paletas mejoradas.  

Se configur√≥ el estilo gr√°fico con `plt.style.use('default')` y la paleta de colores `husl` de seaborn.  

Se utilizaron histogramas con 20 bins, colores personalizados y bordes negros para resaltar la forma de cada distribuci√≥n.  

#### üìà Resultados

Se obtuvieron las  **tres gr√°ficas de distribuci√≥n** que se encuentra como grafica 1 en evidencias:  

- **Edad (Age):** distribuci√≥n etaria entre 18 y 70 a√±os.  
- **Ingresos Anuales (Annual Income):** distribuci√≥n de ingresos de 15k a 137k.  
- **Spending Score (1-100):** dispersi√≥n de h√°bitos de gasto en la escala de 1 a 100.  


#### üîé An√°lisis

- **Edad (Age):** la poblaci√≥n muestra una buena diversidad etaria, con mayor concentraci√≥n entre los 20 y 40 a√±os.  
- **Ingresos Anuales (Annual Income):** se observa una distribuci√≥n amplia, con predominancia en el rango de 40k‚Äì80k, lo que indica variedad en perfiles socioecon√≥micos.  
- **Spending Score:** presenta una distribuci√≥n bastante dispersa, sin concentraci√≥n clara en un solo rango, lo que evidencia distintos patrones de comportamiento en el gasto.  

Estas visualizaciones confirman la **heterogeneidad de la muestra**, lo que es favorable para los an√°lisis de segmentaci√≥n que se realizar√°n en pasos posteriores.

### üîÑ 9. Visualizaciones - Relaciones

En este paso se generaron **gr√°ficos de dispersi√≥n (scatter plots)** para analizar las relaciones entre las variables principales del dataset.

El objetivo fue identificar **patrones de relaci√≥n entre pares de variables** que resulten √∫tiles en procesos de clustering o segmentaci√≥n.

#### üìà Resultados

Se obtuvieron las siguientes tres visualizaciones:  

- **Age vs Income**: relaci√≥n entre la edad y el nivel de ingresos anuales.  
- **Income vs Spending Score**: relaci√≥n fundamental para segmentar clientes por nivel de ingresos y h√°bitos de gasto.  
- **Age vs Spending Score**: relaci√≥n entre edad y puntaje de gasto.  

- **Age vs Income:** no se observa una relaci√≥n lineal clara entre la edad y los ingresos. Los ingresos est√°n repartidos en diferentes rangos de edad.  
- **Income vs Spending Score (CLAVE):** este gr√°fico revela posibles **agrupamientos naturales** en los clientes:  

    - Clientes con **ingresos bajos pero alto gasto**.  
     - Clientes con **ingresos altos pero bajo gasto**.  
    - Grupos intermedios con diferentes combinaciones.  
  
  Esto lo convierte en una variable clave para aplicar algoritmos de clustering.  

- **Age vs Spending Score:** muestra cierta dispersi√≥n heterog√©nea, sin un patr√≥n definido fuerte, aunque podr√≠a sugerir tendencias en algunos rangos etarios.  

Las relaciones exploradas refuerzan que la combinaci√≥n **Ingresos vs Spending Score** es la m√°s prometedora para definir segmentos de clientes en futuros an√°lisis.

### üîó 10. Matriz de Correlaci√≥n

En este paso se calcul√≥ la **matriz de correlaci√≥n** entre las variables cuantitativas principales:  
- **Age**  
- **Annual Income (k$)**  
- **Spending Score (1-100)**  

El objetivo fue identificar si existen relaciones lineales significativas entre estas variables.


#### üìà Resultados

La **matriz de correlaci√≥n** obtenida fue:  

|                            | Age   | Annual Income (k$)  | Spending Score (1-100)  |
|----------------------------|-------|---------------------|-------------------------|
| **Age**                    | 1.000 | -0.012              | -0.327                  |
| **Annual Income (k$)**     | -0.012| 1.000               | 0.010                   |
| **Spending Score (1-100)** | -0.327| 0.010               | 1.000                   |


La correlaci√≥n m√°s fuerte detectada fue:  
üëâ **Annual Income (k$) ‚Üî Spending Score (1-100): 0.010**

- **Age vs Spending Score:** existe una **correlaci√≥n negativa moderada** (-0.327), lo que sugiere que a mayor edad, el puntaje de gasto tiende a ser menor.  
- **Age vs Income:** pr√°cticamente **no hay correlaci√≥n** (-0.012). La edad no determina el nivel de ingresos.  
- **Income vs Spending Score:** la correlaci√≥n es **muy baja (0.010)**, indicando que el ingreso anual no tiene una relaci√≥n lineal clara con el puntaje de gasto.  

Las variables presentan **baja correlaci√≥n lineal entre s√≠**, lo cual puede favorecer la aplicaci√≥n de algoritmos de clustering, ya que cada variable aporta informaci√≥n distinta.

### üë• 11. An√°lisis Comparativo por G√©nero

En este paso se realiz√≥ un **an√°lisis comparativo** entre hombres y mujeres respecto a las variables num√©ricas del dataset.

El objetivo fue identificar diferencias estad√≠sticas relevantes entre ambos g√©neros para enriquecer el proceso de segmentaci√≥n. 

#### üìä Resultados

La tabla con los resultados obtenidos fue la siguiente:  

| Genre  | Age (mean) | Age (std) | Annual Income (mean)  | Annual Income (std)  | Spending Score (mean)  | Spending Score (std)  |
|--------|------------|-----------|-----------------------|----------------------|------------------------|-----------------------|
| Female | 38.10      | 12.64     | 59.25                 | 26.01                | 51.53                  | 24.11                 |
| Male   | 39.81      | 15.51     | 62.23                 | 26.64                | 48.51                  | 27.90                 |

- **Spending Score (1-100):** Las **mujeres** presentan un promedio m√°s alto (diferencia: **3.0**).  
- Para **Age** y **Annual Income (k$)** las diferencias no fueron destacadas en los resultados, pero se aprecia una ligera tendencia de los hombres hacia mayor edad promedio e ingresos anuales.  

Este an√°lisis aporta informaci√≥n clave para comprender posibles diferencias de comportamiento en el gasto seg√∫n g√©nero.

### üìù 12. S√≠ntesis de Insights

En este paso se elabor√≥ una **s√≠ntesis preliminar de los principales hallazgos** obtenidos en los an√°lisis anteriores.  
El objetivo fue consolidar patrones clave, evaluar la relevancia de las variables y preparar el terreno para la etapa de **clustering**.

#### ‚öôÔ∏è Decisiones tomadas

Para completar los espacios en blanco, se tomaron en cuenta los siguientes criterios:  

- **Variable con mayor variabilidad:** se seleccion√≥ **Annual Income (k$)** ya que present√≥ la mayor desviaci√≥n est√°ndar (‚âà 26.26), lo que refleja una dispersi√≥n amplia en los ingresos.  
- **Correlaci√≥n fuerte:** no se identificaron correlaciones fuertes; la m√°s destacada fue **Age ‚Üî Spending Score** con r ‚âà -0.33, considerada moderada y negativa.  
- **Outliers:** mediante el m√©todo **IQR**, se detectaron al menos **2 outliers en Annual Income (k$)**, por lo que se consider√≥ la variable con m√°s valores at√≠picos.  
- **Diferencias por g√©nero:** se complet√≥ con base en los promedios observados: hombres con mayor edad (39.81) e ingresos (62.23 k$), y mujeres con mayor Spending Score (51.53).  
- **Insight m√°s relevante:** se destac√≥ que, aunque la correlaci√≥n Income‚ÄìSpending Score es cercana a cero, en el scatter plot se observaron **grupos naturales** bien definidos.  
- **Variables m√°s importantes para clustering:** se eligieron **Annual Income (k$)** y **Spending Score (1-100)** porque permiten diferenciar mejor los segmentos.  
- **Preparaci√≥n para clustering:** se describi√≥ la relaci√≥n Income‚ÄìSpending Score como pr√°cticamente nula linealmente, pero con evidencia visual de subgrupos; esto justifica su uso en la segmentaci√≥n.  

Esta s√≠ntesis permite **resumir la historia que cuentan los datos** y fundamentar la elecci√≥n de variables para la segmentaci√≥n. Los patrones detectados anticipan que el clustering ser√° una t√©cnica adecuada para identificar **perfiles de clientes diferenciados**.

### üîç 13. Identificaci√≥n de Features para Clustering

En este paso se definieron las **variables (features) que ser√°n utilizadas en el clustering**.  

Se evaluaron todas las columnas del dataset, separando entre variables num√©ricas, categ√≥ricas y aquellas que deben excluirse por no aportar valor al an√°lisis. 

#### üìä Resultados

**AN√ÅLISIS DE COLUMNAS DISPONIBLES:**  

- Todas las columnas: `['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']`
- Num√©ricas detectadas: `['CustomerID', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']`  
- Categ√≥ricas detectadas: `['Genre']`  

**SELECCI√ìN DE FEATURES PARA CLUSTERING:**  

- Excluidas: `['CustomerID']` ‚Äì no aporta informaci√≥n.  
- Num√©ricas elegidas: `['Age', 'Annual Income (k$)', 'Spending Score (1-100)']`.  
- Categ√≥ricas elegidas: `['Genre']` ‚Äì ser√° codificada antes del an√°lisis.  

La selecci√≥n de features asegura que el clustering se base en **atributos demogr√°ficos, econ√≥micos y de comportamiento** de los clientes, descartando informaci√≥n irrelevante.  

Esto prepara el dataset para la pr√≥xima etapa: **normalizaci√≥n y codificaci√≥n de variables** antes de aplicar los algoritmos de segmentaci√≥n.  

### üî° 14. Codificaci√≥n de Variables Categ√≥ricas con OneHotEncoder

En este paso se realiz√≥ la **codificaci√≥n de la variable categ√≥rica `Genre`** utilizando `OneHotEncoder` de `sklearn.preprocessing`.  

El objetivo fue transformar los valores de texto ("Male", "Female") en columnas num√©ricas binarias, asegurando compatibilidad con los algoritmos de clustering y machine learning.

#### ‚öôÔ∏è Decisiones tomadas

Para completar los espacios en blanco y configurar correctamente el encoder se consideraron las siguientes razones:  

- **OneHotEncoder (de sklearn.preprocessing):** elegido por su integraci√≥n con pipelines, consistencia en flujos de ML y capacidad de manejar categor√≠as nuevas autom√°ticamente.  
- **M√©todo `fit_transform`:** usado para **ajustar y transformar** los datos categ√≥ricos en un solo paso, como recomienda la documentaci√≥n oficial.  
- **M√©todo `get_feature_names_out`:** seleccionado para extraer los nombres de las nuevas columnas generadas (`Genre_Female`, `Genre_Male`), lo que asegura trazabilidad y claridad en los resultados.  

#### üìä Resultados

**CODIFICACI√ìN DE VARIABLES CATEG√ìRICAS CON SKLEARN:**  
- Categor√≠as originales: `['Male', 'Female']`  
- Columnas generadas: `['Genre_Female', 'Genre_Male']`  
- Shape: `(200, 1) ‚Üí (200, 2)`  

**EJEMPLO DE TRANSFORMACI√ìN:**  

| Genre  | Genre_Female | Genre_Male |
|--------|--------------|------------|
| Male   | 0.0          | 1.0        |
| Male   | 0.0          | 1.0        |
| Female | 1.0          | 0.0        |
| Female | 1.0          | 0.0        |
| Female | 1.0          | 0.0        |

La codificaci√≥n con `OneHotEncoder` permiti√≥ transformar la variable categ√≥rica `Genre` en un **formato num√©rico interpretable por los algoritmos de clustering**, manteniendo la consistencia con pipelines de `sklearn`.  

Esto asegura que tanto **g√©nero masculino como femenino** sean representados como **features independientes**, listas para integrarse en el modelo.  

### üõ†Ô∏è 15. Preparaci√≥n del Dataset Final

En este paso se realiz√≥ la **construcci√≥n del dataset final** que se utilizar√° para los an√°lisis de clustering, combinando las variables num√©ricas originales con las variables categ√≥ricas codificadas.

El objetivo fue preparar un dataset **limpio, completo y listo para modelado**, asegurando que todas las columnas est√©n en formatos adecuados y que no existan valores faltantes.

#### üìã Resultados

| Age | Annual Income (k$)| Spending Score (1-100)| Genre_Female  | Genre_Male  |
| --- | ----------------- | --------------------- | ------------- | ----------- |
| 19  | 15                | 39                    | 0.0           | 1.0         |
| 21  | 15                | 81                    | 0.0           | 1.0         |
| 20  | 16                | 6                     | 1.0           | 0.0         |
| 23  | 16                | 77                    | 1.0           | 0.0         |
| 31  | 17                | 40                    | 1.0           | 0.0         |

* Todas las variables se encuentran **listas para clustering**, con los tipos de datos correctos.
* La ausencia de datos faltantes asegura que los algoritmos de segmentaci√≥n no se ver√°n afectados por valores nulos.
* La combinaci√≥n de variables num√©ricas y categ√≥ricas codificadas permite que los clusters capturen **tanto diferencias demogr√°ficas como patrones de gasto**.

---

### ‚úÖ 16. Verificaci√≥n de Calidad de Datos

En este paso se realiz√≥ una **revisi√≥n integral de la calidad del dataset final**, asegurando que los datos est√©n completos, consistentes y listos para los an√°lisis de clustering y normalizaci√≥n.

El objetivo fue detectar posibles **valores faltantes, inconsistencias o errores de tipo** antes de continuar con los pasos de modelado y escalado.

#### üìã Resultados

**Estad√≠sticas por variable:**

* **Age:** Rango 18 - 70, Media 38.9, Desviaci√≥n 14.0  
* **Annual Income (k$):** Rango 15 - 137, Media 60.6, Desviaci√≥n 26.3  
* **Spending Score (1-100):** Rango 1 - 99, Media 50.2, Desviaci√≥n 25.8  
* **Genre_Female:** Rango 0 - 1, Media 0.6, Desviaci√≥n 0.5  
* **Genre_Male:** Rango 0 - 1, Media 0.4, Desviaci√≥n 0.5


* Todas las variables est√°n **completas y sin valores faltantes**, lo que asegura integridad para el an√°lisis posterior.  
* Los tipos de datos son consistentes con su naturaleza (num√©ricas y binarias), permitiendo aplicar t√©cnicas de escalado y clustering sin problemas.  
* Las estad√≠sticas muestran que las variables num√©ricas tienen rangos y dispersi√≥n variados, lo que **justifica la necesidad de normalizaci√≥n** en los pr√≥ximos pasos.  

### ‚öñÔ∏è 17. An√°lisis de Escalas (Pre-Normalizaci√≥n)

En este paso se evaluaron **las escalas y variabilidades de las variables num√©ricas** antes de aplicar cualquier t√©cnica de normalizaci√≥n.  

El objetivo fue **identificar diferencias de rango y dispersi√≥n** entre las variables, lo que es cr√≠tico para algoritmos basados en distancia como K-Means, donde una variable con mayor escala puede dominar la formaci√≥n de clusters.

#### ‚öôÔ∏è Decisiones tomadas

* Se calcularon **estad√≠sticas descriptivas** (m√≠nimo, m√°ximo, media y desviaci√≥n est√°ndar) para cada variable num√©rica (`Age`, `Annual Income (k$)`, `Spending Score (1-100)`).  
* Se completaron los espacios en blanco del an√°lisis para interpretar correctamente los resultados y preparar la justificaci√≥n de la normalizaci√≥n:

      - **Variable con mayor rango:** `Annual Income (k$)` (15‚Äì137; Œî‚âà122)  
      - **Distribuci√≥n de g√©nero:** 56% Female (112) y 44% Male (88)  
      - **Variable con mayor variabilidad (std):** `Annual Income (k$)` (std ‚âà 26.26)  
      - **Edad promedio de los clientes:** ‚âà 38.9 a√±os ‚Üí adultos de mediana edad, tirando a j√≥venes  
      - **Clase social promedio seg√∫n income:** Ingreso medio ‚âà 60.6 k$ ‚Üí clase media  
      - **Importancia de la normalizaci√≥n:** Las escalas y varianzas difieren mucho entre a√±os, k$ y puntaje; m√©todos basados en distancia son sensibles a escala, por lo que sin normalizar, `Annual Income` dominar√≠a los clusters.

* Se guardaron los nombres de todas las columnas (`feature_columns`) para la **fase de preparaci√≥n de datos**.

* La variable con **mayor rango y dispersi√≥n** es `Annual Income (k$)`, lo que puede sesgar los algoritmos de clustering sin normalizaci√≥n.  
* La **distribuci√≥n de g√©nero** es relativamente equilibrada, aunque ligeramente m√°s femenina.  
* La **edad promedio** indica que la mayor√≠a de los clientes son adultos j√≥venes/mediana edad.  
* El **income promedio** sugiere que la muestra pertenece mayoritariamente a clase media.  
* La **normalizaci√≥n ser√° cr√≠tica** para que todas las variables contribuyan de manera equitativa a la segmentaci√≥n.

---

### ‚öôÔ∏è 18. Setup para Normalizaci√≥n

En este paso se prepararon las **herramientas de normalizaci√≥n** que se evaluar√°n para el dataset, con el objetivo de que todas las variables contribuyan de manera equitativa al an√°lisis de clustering.

El objetivo fue establecer un **setup comparativo** para seleccionar el scaler m√°s adecuado entre MinMax, Standard y Robust, considerando las diferencias de escala entre las variables.

#### ‚öôÔ∏è Decisiones tomadas

* Se importaron los tres scalers m√°s comunes de `sklearn.preprocessing`:  
  - `MinMaxScaler`: transforma los datos para que est√©n dentro de un rango fijo (por ejemplo, 0‚Äì1).  
  - `StandardScaler`: estandariza los datos centrando en 0 y escalando a desviaci√≥n est√°ndar 1.  
  - `RobustScaler`: similar a StandardScaler, pero utiliza mediana y rango intercuart√≠lico, robusto frente a outliers.  

* Se completaron los espacios en blanco con estos scalers porque **queremos probar las tres t√©cnicas m√°s habituales** y elegir la que mejor se adapte a la dispersi√≥n y rangos de nuestras variables.  
* Se revisaron las **escalas actuales de cada variable** para resaltar la gran diferencia entre rangos y justificar la necesidad de normalizaci√≥n.

#### üìã Resultados

**Escalas actuales del dataset:**

* **Age:** 18.0 ‚Äì 70.0 (rango: 52.0)  
* **Annual Income (k$):** 15.0 ‚Äì 137.0 (rango: 122.0)  
* **Spending Score (1-100):** 1.0 ‚Äì 99.0 (rango: 98.0)  
* **Genre_Female:** 0.0 ‚Äì 1.0 (rango: 1.0)  
* **Genre_Male:** 0.0 ‚Äì 1.0 (rango: 1.0)  

* Las escalas son **muy diferentes**, por lo que los algoritmos de clustering basados en distancia ser√≠an dominados por `Annual Income (k$)` si no se aplica normalizaci√≥n.  
* La normalizaci√≥n es **cr√≠tica** para garantizar que cada variable tenga un peso equilibrado en la segmentaci√≥n de clientes.

---

### ‚öñÔ∏è 19. Aplicar los 3 Scalers

En este paso se **aplicaron los tres scalers seleccionados** al dataset final para generar versiones normalizadas de los datos, comparando su comportamiento y efectos en la escala de las variables.

El objetivo fue **preparar tres datasets escalados** que se utilizar√°n en an√°lisis de clustering y para decidir cu√°l scaler es m√°s adecuado seg√∫n la distribuci√≥n y sensibilidad a outliers.

#### ‚öôÔ∏è Decisiones tomadas

* Se crearon los tres scalers m√°s utilizados de `sklearn.preprocessing`:
  - `MinMaxScaler()`: escala cada variable al rango [0,1], √∫til cuando se desea conservar proporciones relativas.  
  - `StandardScaler()`: estandariza centrando en 0 y con desviaci√≥n est√°ndar 1, recomendado para algoritmos sensibles a la magnitud relativa de las variables.  
  - `RobustScaler()`: utiliza la mediana y el rango intercuart√≠lico (IQR), lo que lo hace **robusto frente a outliers**.  

* Se completaron los espacios en blanco con estas clases porque **son los scalers indicados seg√∫n la documentaci√≥n y las caracter√≠sticas del dataset** (rangos muy distintos y presencia potencial de valores extremos).  

* Se utiliz√≥ el m√©todo `fit_transform()` para cada scaler, ya que este **entrena y transforma los datos en un solo paso**, simplificando el workflow y asegurando consistencia entre entrenamiento y transformaci√≥n.

#### üìã Resultados

**Aplicaci√≥n de los scalers:**

* **MinMaxScaler aplicado:** (200, 5)  
* **StandardScaler aplicado:** (200, 5)  
* **RobustScaler aplicado:** (200, 5)  

* Tenemos **3 versiones escaladas del dataset** listas para comparar visualmente y evaluar cu√°l mantiene mejor la estructura de los datos sin que ninguna variable domine indebidamente la formaci√≥n de clusters.

### üîë 20. Comparaci√≥n Visual ‚Äì Boxplots

En este paso se realiz√≥ una **comparaci√≥n visual de diferentes t√©cnicas de escalado** aplicadas a las variables del dataset.  

El objetivo fue observar c√≥mo cada m√©todo transforma la escala de los datos y analizar su impacto en la distribuci√≥n de las variables.  

#### üìà Resultados

Se generaron **4 boxplots** (ver imagen en evidencias como grafica 4):  

1. **Original:** las variables presentan rangos y escalas muy diferentes (ejemplo: Age frente a Income).  
2. **MinMaxScaler:** ajusta todas las variables al rango [0,1], √∫til para modelos sensibles a la magnitud.  
3. **StandardScaler:** centra en media 0 y escala en desviaci√≥n est√°ndar 1; las distribuciones se vuelven comparables.  
4. **RobustScaler:** reduce el impacto de outliers al usar mediana y rango intercuart√≠lico.  

- La comparaci√≥n muestra claramente que **cada scaler ajusta de manera diferente las variables**.  
- **MinMaxScaler** homogeniza escalas pero puede ser sensible a outliers.  
- **StandardScaler** es adecuado cuando se asume normalidad en las variables.  
- **RobustScaler** es m√°s resistente a valores extremos, preservando mejor la forma de las distribuciones.  

- Esta visualizaci√≥n permite **elegir el m√©todo de escalado m√°s adecuado** seg√∫n el algoritmo de clustering que se utilizar√° posteriormente.

### üìä 21. Comparaci√≥n de Distribuciones

En este paso se analiz√≥ en detalle la variable **Annual Income (k$)**, comparando su distribuci√≥n en la escala original y tras aplicar diferentes t√©cnicas de normalizaci√≥n.  

El objetivo fue observar c√≥mo los distintos **scalers** modifican la forma y rango de la variable, manteniendo o alterando patrones de dispersi√≥n.  

Se compararon:  

- **Original (sin escalar)**  
- **MinMaxScaler**  
- **StandardScaler**  
- **RobustScaler**    

#### üìà Resultados

Se obtuvieron **4 histogramas comparativos** (ver imagen en evidencias como gr√°fica 5):  

1. **Original:** rango entre 15k y 137k, con mayor concentraci√≥n entre 40k y 80k.  
2. **MinMaxScaler:** normaliza al rango [0,1], conservando la forma original pero reduciendo magnitudes.  
3. **StandardScaler:** centra en 0 y escala por desviaci√≥n est√°ndar, distribuyendo valores en torno a la media.  
4. **RobustScaler:** reduce el efecto de outliers, ajustando la variable en torno a la mediana y el IQR.  

- La **forma general de la distribuci√≥n se mantiene**, pero el rango cambia significativamente seg√∫n el m√©todo.  
- **MinMax** es m√°s adecuado cuando se requiere un rango fijo (ejemplo: algoritmos basados en distancia como K-Means).  
- **Standard** facilita la comparaci√≥n entre variables normalizadas, ideal para modelos que asumen distribuci√≥n gaussiana.  
- **Robust** es recomendable en presencia de **outliers**, ya que limita su influencia.  

- Esta comparaci√≥n confirma que la elecci√≥n del **scaler** depende del algoritmo a utilizar y de la naturaleza de los datos.

### üìë  22. An√°lisis Estad√≠stico Post-Scaling

En este paso se calcularon las **medidas estad√≠sticas b√°sicas** de la variable **Annual Income (k$)** en su forma original y tras aplicar distintos m√©todos de escalado.  

Se evaluaron: **media, desviaci√≥n est√°ndar, m√≠nimo y m√°ximo** para cada caso.

#### üìà Resultados obtenidos

| Versi√≥n         | Media  | Std   | Min    | Max    |
|-----------------|--------|-------|--------|--------|
| **Original**    | 60.6   | 26.3  | 15.0   | 137.0  |
| **MinMax**      | 0.373  | 0.215 | 0.000  | 1.000  |
| **Standard**    | -0.000 | 1.000 | -1.739 | 2.918  |
| **Robust**      | -0.026 | 0.718 | -1.274 | 2.068  |

- **MinMaxScaler**  
      - Ajusta los valores al rango **[0,1]**.  
      - Conserva las proporciones originales, pero cambia las magnitudes.  

- **StandardScaler**  
      - Normaliza los datos para que tengan **media = 0** y **desviaci√≥n est√°ndar = 1**.  
      - Puede generar valores negativos y positivos dependiendo de la distancia a la media.  

- **RobustScaler**  
      - Centra en la mediana y escala en funci√≥n del **IQR (rango intercuart√≠lico)**.  
      - Resulta menos afectado por **outliers**, generando un rango m√°s controlado.  

‚úÖ Este an√°lisis estad√≠stico confirma c√≥mo **cada t√©cnica de escalado modifica la variable**, y por qu√© la elecci√≥n del m√©todo debe depender del algoritmo y del comportamiento de los datos.

### ü§ñ 23. Test de Impacto en Clustering

En este paso se evalu√≥ c√≥mo **el tipo de escalado de los datos influye en la calidad del clustering**. 

Se utiliz√≥ el algoritmo **K-Means (K=4)** junto con la m√©trica **Silhouette Score** para comparar resultados.  

#### ‚öôÔ∏è Decisiones tomadas

1. **`from sklearn.cluster import KMeans`**  

       * Se eligi√≥ **KMeans** como algoritmo de clustering porque la consigna ped√≠a este m√©todo expl√≠citamente.  

2. **`from sklearn.metrics import silhouette_score`**  

       * Se us√≥ **Silhouette Score** ya que es la m√©trica est√°ndar para medir la **cohesi√≥n y separaci√≥n** de clusters (rango [-1, 1]).  

3. **`kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)`**  

       * `n_clusters=4`: valor de K definido en la consigna.  
       * `random_state=42`: asegura reproducibilidad de resultados.  
       * `n_init=10`: ejecuta el algoritmo varias veces para evitar caer en m√≠nimos locales.  

4. **`labels = kmeans.fit_predict(X_scaled_data)`**  

       * Se utiliz√≥ **fit_predict()** porque devuelve directamente las **etiquetas 1D de los clusters**.  
       * La alternativa `fit_transform()` hubiera devuelto distancias a los centroides (matriz 2D), lo cual no era √∫til para este an√°lisis.  

5. **`silhouette = silhouette_score(X_scaled_data, labels)`**  

       * Eval√∫a la calidad del clustering.  
       * **Valores m√°s altos ‚Üí clusters mejor definidos y separados.**  

#### üìë Resultados obtenidos

**QUICK TEST: Impacto en Clustering (K=4)**

  - MinMax: Silhouette Score = 0.364
  - Standard: Silhouette Score = 0.332
  - Robust: Silhouette Score = 0.298

**GANADOR: MinMax (Score: 0.364)**

- **MinMaxScaler** fue el que mejor desempe√±o mostr√≥ en clustering con un **silhouette score de 0.364**.  
- **StandardScaler** qued√≥ en segundo lugar.  
- **RobustScaler** tuvo el score m√°s bajo.  

- El **escalado tiene un impacto directo en clustering**, ya que K-Means depende de distancias euclidianas.  
- En este caso:  

      - **MinMaxScaler** funcion√≥ mejor porque comprime todas las variables en el mismo rango [0,1], evitando que alguna con mayor varianza domine el clustering.  
      - **StandardScaler** mejora respecto a la escala original, pero no alcanz√≥ el desempe√±o de MinMax.  
      - **RobustScaler** pierde calidad al centrarse en la mediana/IQR, lo cual reduce sensibilidad a valores extremos, pero tambi√©n la capacidad de separaci√≥n.  

‚úÖ Conclui que para este dataset, **MinMaxScaler es la mejor opci√≥n previa a clustering con K-Means**.

### üéØ 24. Decisi√≥n Final de Scaler

En este paso se selecciona el **scaler definitivo** para preparar los datos antes de aplicar **PCA y Feature Selection**.  
La decisi√≥n se toma en base a los resultados de los **silhouette scores** obtenidos en el test de impacto en clustering.  


#### ‚öôÔ∏è Decisiones tomadas

**`selected_scaler_name = best_scaler`**  

   - Se selecciona autom√°ticamente el **scaler con mejor desempe√±o seg√∫n silhouette score**.  
   - En este caso: **MinMaxScaler**.  

 **Elecci√≥n de MinMaxScaler**  

   - Escala todos los valores al mismo rango **[0,1]**.  
   - Esto balancea las variables, evitando que alguna con mayor rango domine la distancia euclidiana en K-Means.  
   - Permiti√≥ una **mejor separaci√≥n de clusters**.  

**Problemas con otros scalers**  

   - **StandardScaler**: al estandarizar con media 0 y std 1, a√∫n puede sufrir cuando los rangos de las variables son muy distintos.  
   - **RobustScaler**: reduce el impacto de outliers, pero en este dataset eso hizo perder sensibilidad para separar bien los clusters.  

#### üìë Resultados obtenidos

**COMPLETE TU AN√ÅLISIS:**
  
  -  Mejor scaler seg√∫n silhouette: MinMax
  - ¬øPor qu√© crees que funcion√≥ mejor? MinMaxScaler funcion√≥ mejor porque todos los valores se escalaron al mismo rango [0,1], lo que facilita que K-Means detecte la estructura de los clusters, evitando que variables con rangos grandes dominen la distancia.
  -  ¬øAlg√∫n scaler tuvo problemas obvios? RobustScaler y StandardScaler tuvieron scores m√°s bajos; RobustScaler es m√°s conservador con outliers y StandardScaler puede verse afectado por rangos muy distintos entre variables, reduciendo la separaci√≥n de clusters visibles.

**SCALER SELECCIONADO: MinMax**

- Datos preparados: (200, 5)
- Listo para PCA y Feature Selection

El **scaler final elegido es MinMaxScaler**, con lo cual los datos quedan listos para aplicar t√©cnicas de **reducci√≥n de dimensionalidad (PCA)** y **selecci√≥n de caracter√≠sticas**.  

Esto garantiza que todas las variables contribuyan de manera equilibrada al an√°lisis posterior.  

### üèÜ 25. Decisi√≥n Final de Scaler

En este paso se tom√≥ la **decisi√≥n definitiva sobre qu√© scaler utilizar** para continuar con el pipeline de preprocesamiento antes de aplicar PCA y selecci√≥n de caracter√≠sticas.

El criterio principal fue el **√≠ndice de silhouette**, una m√©trica que mide la calidad de la separaci√≥n de los clusters, complementado con un an√°lisis conceptual de c√≥mo cada scaler afect√≥ la estructura de los datos.

#### ‚öôÔ∏è Decisiones tomadas

* El espacio en blanco `"¬øPor qu√© crees que funcion√≥ mejor?"` se complet√≥ con la explicaci√≥n de que **MinMaxScaler funcion√≥ mejor porque ajusta todas las variables al mismo rango [0,1]**, evitando que variables con escalas m√°s amplias dominen las distancias. Esto es especialmente beneficioso para algoritmos como **K-Means**, que dependen fuertemente de las distancias euclidianas.

* El espacio en blanco `"¬øAlg√∫n scaler tuvo problemas obvios?"` se complet√≥ se√±alando que:
  - **RobustScaler** fue demasiado conservador debido a que prioriza la mediana y el IQR, lo que puede aplanar diferencias importantes en datasets donde los outliers no son un problema central.  
  - **StandardScaler** puede ser afectado por variables con rangos muy distintos, lo que reduce la separaci√≥n de clusters detectada.  

* Se seleccion√≥ como scaler final **MinMaxScaler**, siguiendo tanto la evidencia emp√≠rica (mejor score de silhouette) como el razonamiento te√≥rico.


### üìä 26. PCA - Reducci√≥n de Dimensionalidad

En este paso se aplic√≥ **An√°lisis de Componentes Principales (PCA)** con el objetivo de reducir la dimensionalidad del dataset de **5 variables originales** a **2 componentes principales**, facilitando la **visualizaci√≥n** y el **an√°lisis de la varianza explicada**.

#### üìà Resultados

**Varianza explicada por cada componente:**

| Componente | Varianza Explicada | Varianza (%) | Varianza Acumulada | Acumulada (%) |
| ---------- | ------------------ | ------------ | ------------------ | ------------- |
| PC1        | 0.726              | 72.6%        | 0.726              | 72.6%         |
| PC2        | 0.137              | 13.7%        | 0.863              | 86.3%         |
| PC3        | 0.070              | 7.0%         | 0.932              | 93.2%         |
| PC4        | 0.068              | 6.8%         | 1.000              | 100.0%        |
| PC5        | 0.000              | 0.0%         | 1.000              | 100.0%        |

* **Para 90% de varianza retenida ‚Üí 3 componentes.**
* **Para 95% de varianza retenida ‚Üí 4 componentes.**
* **Para visualizaci√≥n ‚Üí 2 componentes (86.3%).**

- Ver gr√°fica 6 en evidencias

* **PC1 (72.6% de varianza):** est√° fuertemente influenciado por las variables **Genre_Female (-0.706)** y **Genre_Male (0.706)**, representando principalmente la **diferenciaci√≥n por g√©nero**.
* **PC2 (13.7% de varianza):** refleja un contraste entre **Age (0.727)** y **Spending Score (-0.685)**, diferenciando clientes m√°s j√≥venes con mayor gasto de aquellos mayores con menor score.


* El **primer componente (PC1)** capta la mayor parte de la variabilidad, diferenciando principalmente por **g√©nero**.
* El **segundo componente (PC2)** aporta un eje adicional que discrimina entre clientes seg√∫n la relaci√≥n entre **edad** y **h√°bitos de gasto**.

* Se gener√≥ un grafico (ver en evidencias como gr√°fica 7) que respresenta esto.
* La proyecci√≥n en 2D permite una visualizaci√≥n clara de la estructura de los datos, preservando la mayor√≠a de la informaci√≥n relevante (**86.3% de varianza**).

   üéØ PC1 parece representar: Comportamiento de gasto vs ingreso (Income y Spending Score dominan)
   
   üéØ PC2 parece representar: Edad y g√©nero de los clientes
   
   üìä Los clusters visibles sugieren: Diferentes segmentos de clientes, por ejemplo, j√≥venes con alto gasto, mayores con bajo gasto, etc.

### üõ†Ô∏è 28. Imports y Setup - Feature Selection

En este paso se configuraron las herramientas necesarias para la Selecci√≥n de Caracter√≠sticas, espec√≠ficamente utilizando la t√©cnica de Forward/Backward Selection disponible en scikit-learn.

#### ‚öôÔ∏è Decisiones tomadas

La elecci√≥n de SequentialFeatureSelector se debe a que la pista indicaba expl√≠citamente la documentaci√≥n de esta clase.

Este m√©todo permite seleccionar un subconjunto de variables siguiendo un procedimiento secuencial, ya sea:

- Forward Selection: comienza con 0 variables e incorpora aquellas que mejoran el modelo.

- Backward Selection: comienza con todas las variables y elimina las menos relevantes.


### ‚öôÔ∏è 30. Setup y Funci√≥n de Evaluaci√≥n

En este paso se prepar√≥ la **infraestructura para comparar dos enfoques de reducci√≥n de dimensionalidad**:  

- **Feature Selection (Forward y Backward Selection)**  
- **PCA (Principal Component Analysis)**  

El objetivo es determinar si conviene **seleccionar un subconjunto de variables originales** o **transformarlas en nuevas combinaciones lineales**.  

Para ello, se defini√≥ una **funci√≥n de evaluaci√≥n basada en clustering (K-Means + Silhouette Score)** y se cre√≥ un **estimador personalizado compatible con scikit-learn**, que permitir√° usar t√©cnicas como `SequentialFeatureSelector`.

#### üìà Resultados

- Se imprimi√≥ un resumen inicial mostrando que se trabajar√° con **200 muestras y 5 features**.  


### ‚öôÔ∏è 31. Baseline - Todas las Features

En este paso se calcul√≥ el **Silhouette Score usando todas las features disponibles** (ya escaladas con el scaler seleccionado).  
El objetivo es obtener una **l√≠nea base (baseline)** que permita luego comparar el desempe√±o de m√©todos de **selecci√≥n de caracter√≠sticas (Forward/Backward)** y **transformaci√≥n (PCA)**.  

#### üìà Resultados

- **Silhouette Score baseline:** `0.364`  
- N√∫mero de features usadas: `5` (todas las originales tras escalado).  
- El print adicional plantea la hip√≥tesis: *‚Äú¬øPodremos mejorar seleccionando solo las mejores 3?‚Äù*.  

#### üîé An√°lisis

- Un **Silhouette Score de 0.364** indica una **separaci√≥n moderada de clusters**: los grupos son distinguibles, pero no est√°n completamente bien definidos.  
- Es probable que existan **features redundantes o ruidosas**, lo que motiva probar **feature selection** para mejorar la cohesi√≥n y separaci√≥n de los clusters.  
- Este baseline es clave: servir√° como **punto de comparaci√≥n** para evaluar si **Forward Selection, Backward Selection o PCA** logran un mejor puntaje reduciendo el n√∫mero de dimensiones.  

### ‚öôÔ∏è 32. Forward Selection

En este paso se aplic√≥ **Forward Feature Selection** utilizando `SequentialFeatureSelector` de `sklearn`.  
El objetivo fue seleccionar las **3 mejores features** para clustering, comparando su desempe√±o contra el baseline obtenido con las 5 originales.  

#### ‚öôÔ∏è Decisiones tomadas

- Se us√≥ la clase `SequentialFeatureSelector` porque permite realizar selecci√≥n secuencial de caracter√≠sticas.  
- El estimador fue el **ClusteringEstimator**, implementado previamente, compatible con `fit()` y `score()` para Silhouette Score.  
- Par√°metros configurados:  
  - `n_features_to_select=3` ‚Üí se busc√≥ reducir de 5 a 3 features.  
  - `direction='forward'` ‚Üí se parte de 0 features y se agregan progresivamente las que m√°s mejoran el score.  
  - `cv=3` y `n_jobs=-1` ‚Üí se habilit√≥ validaci√≥n cruzada y uso de todos los n√∫cleos disponibles para optimizar el proceso.  
- Para entrenar el selector se utiliz√≥ `.fit(X_preprocessed)`.  
- Para obtener la m√°scara booleana de features seleccionadas se us√≥ `.get_support()`.  

#### üìà Resultados

- **Features seleccionadas:** `['Spending Score (1-100)', 'Genre_Female', 'Genre_Male']`  
- **Silhouette Score:** `0.573`  
- Comparaci√≥n: **baseline = 0.364 ‚Üí forward = 0.573**  
- Resultado: **‚úÖ Mejora significativa** al reducir de 5 a 3 features.  

#### üîé An√°lisis

- Forward Selection identific√≥ que las variables de **g√©nero** y **Spending Score** aportan m√°s a la separaci√≥n de clusters que la edad o el ingreso.  
- El aumento del **Silhouette Score de 0.364 a 0.573** confirma que algunas variables originales introduc√≠an ruido y reduc√≠an la cohesi√≥n de los clusters.  
- Esta t√©cnica demuestra la **utilidad de la selecci√≥n de features** en clustering no supervisado, donde no existe una variable target clara.  

### ‚öôÔ∏è 33. Backward Elimination

En este paso se aplic√≥ **Backward Feature Elimination** utilizando `SequentialFeatureSelector` de `sklearn`.  
El objetivo fue seleccionar las **3 mejores features** partiendo del conjunto completo y eliminando progresivamente las menos relevantes.  

#### ‚öôÔ∏è Decisiones tomadas

- Se utiliz√≥ la clase `SequentialFeatureSelector`, igual que en Forward Selection.  
- El estimador fue nuevamente el **ClusteringEstimator**, que implementa `fit()` y `score()` para calcular el **Silhouette Score**.  
- Par√°metros configurados:  
   
  - `direction='backward'` ‚Üí a diferencia de Forward, aqu√≠ se comienza con **todas las features** y se van eliminando las menos relevantes en cada paso.  
- Para entrenar se utiliz√≥ `.fit(X_preprocessed)`.  
- Para obtener la m√°scara booleana de features seleccionadas se emple√≥ `.get_support()`.  

#### üìà Resultados

- **Features seleccionadas:** `['Spending Score (1-100)', 'Genre_Female', 'Genre_Male']`  
- **Silhouette Score:** `0.573`  
- Comparaci√≥n: **baseline = 0.364 ‚Üí backward = 0.573**  
- Resultado: **‚úÖ Mejora significativa** respecto al baseline.  

#### üîé An√°lisis

- Backward Elimination lleg√≥ al **mismo subconjunto de features** que Forward Selection, confirmando que estas 3 variables son las m√°s relevantes para separar clusters en este dataset.  
- El incremento del **Silhouette Score (+0.209 sobre el baseline)** demuestra que eliminar variables poco informativas (como Edad e Ingreso Anual) mejora la cohesi√≥n y separaci√≥n de clusters.  
- Forward y Backward, aunque conceptualmente opuestos, convergieron en la misma soluci√≥n, lo cual otorga **robustez y confianza** al resultado obtenido.  

### ü§ñ 34. Comparaci√≥n Final de M√©todos

En este paso realizamos una comparaci√≥n directa de todos los enfoques aplicados: baseline, selecci√≥n hacia adelante, eliminaci√≥n hacia atr√°s y reducci√≥n dimensional con PCA.

El objetivo es identificar qu√© t√©cnica logra el mejor desempe√±o de clustering seg√∫n el **Silhouette Score** y cuantificar la mejora respecto al baseline.

üìä **Resultados obtenidos**

- üèÅ **Baseline (todas)**: 0.364  
- üîÑ **Forward Selection**: 0.573  
- üîô **Backward Elimination**: 0.573  
- üìê **PCA (2D)**: 0.686  

üèÜ **Ganador**: **PCA (2D)** con un **Silhouette Score = 0.686**

üîç **An√°lisis comparativo**

- PCA (2D): 0.686 (**+88.3%** vs baseline)  
- Forward Selection: 0.573 (**+57.5%** vs baseline)  
- Backward Elimination: 0.573 (**+57.5%** vs baseline)  
- Baseline (todas): 0.364 (**+0.0%**)  

 Aunque tanto *Forward* como *Backward* mejoran notablemente respecto al baseline, la reducci√≥n dimensional con **PCA a 2 componentes** logra el mejor rendimiento global, confirmando que una buena transformaci√≥n de features puede superar la selecci√≥n secuencial.

### üìä 35. Visualizaci√≥n Comparativa 

En este paso se realiz√≥ una **comparaci√≥n visual** entre distintos m√©todos de **reducci√≥n de dimensionalidad y selecci√≥n de caracter√≠sticas** aplicados al dataset.

#### üìà Resultados

Se obtuvo la gr√°fica comparativa (evidencia **Gr√°fico 8**), donde se visualizan los puntajes alcanzados:

* **Baseline:** `0.364`
* **Forward Selection:** `0.573`
* **Backward Elimination:** `0.573`
* **PCA 2D:** `0.686`

* El **Baseline** mostr√≥ un desempe√±o bajo (`0.364`), por debajo del umbral aceptable.
* Tanto **Forward** como **Backward Selection** alcanzaron un puntaje **aceptable** (`0.573`), mostrando mejoras respecto al baseline.
* El m√©todo **PCA 2D** result√≥ ser el m√°s efectivo, con un puntaje de `0.686`, **cercano al umbral de muy bueno (0.7)**.

Esto indica que **la reducci√≥n de dimensionalidad con PCA en 2D ofrece la mejor representaci√≥n de los datos** para los fines de clustering, superando a los m√©todos tradicionales de selecci√≥n de caracter√≠sticas.

### üéØ 36. An√°lisis y Decisi√≥n Final

En este paso se realiz√≥ un **an√°lisis comparativo** de los m√©todos de selecci√≥n de caracter√≠sticas y reducci√≥n de dimensionalidad, con el fin de tomar una decisi√≥n final sobre cu√°l ofrece el mejor rendimiento para el clustering.

#### ‚öôÔ∏è Decisiones tomadas

* Se listaron expl√≠citamente las **features seleccionadas por Forward y Backward Selection** para verificar coincidencias.
* Se construyeron conjuntos (`set`) para comparar los resultados y comprobar si ambos m√©todos coincid√≠an en las mismas caracter√≠sticas.
* Se respondieron las **preguntas de an√°lisis**, rellenando los espacios en blanco en base a los resultados obtenidos:
  * **Mejor score:** PCA 2D con `0.686`, al ser el valor m√°s alto.
  * **Coincidencia entre Forward y Backward:** S√≠, seleccionaron exactamente las mismas features.
  * **Competitividad del PCA 2D:** S√≠, porque super√≥ a ambos m√©todos de selecci√≥n de features.
  * **Threshold superado:** S√≠, tanto Forward, Backward como PCA 2D superaron el umbral de `0.5`.
  * **Impacto de la reducci√≥n de dimensionalidad:** S√≠, PCA 2D logr√≥ el mayor Silhouette Score, confirmando su efectividad.

#### üìà Resultados

Se obtuvieron los siguientes hallazgos:  

* **Forward Selection:** seleccion√≥ `[Spending Score (1-100), Genre_Female, Genre_Male]`.  
* **Backward Elimination:** seleccion√≥ `[Spending Score (1-100), Genre_Female, Genre_Male]`.  
* **Coincidencias:** ambos m√©todos coincidieron completamente en las features seleccionadas.  

* El **PCA 2D** fue el m√©todo con **mejor desempe√±o** (`0.686`), cercano al umbral de *muy bueno* (`0.7`).  
* **Forward y Backward Selection coincidieron** en las mismas variables, logrando un desempe√±o aceptable (`0.573`).  
* La **reducci√≥n de dimensionalidad** no solo fue competitiva, sino que tambi√©n result√≥ la **estrategia m√°s efectiva** en este caso.  
* Se confirma que la representaci√≥n en **2 componentes principales mejora el clustering** respecto al baseline y a los m√©todos de selecci√≥n cl√°sica.  

En conclusi√≥n, **PCA 2D es la estrategia recomendada** para este dataset, ya que optimiza la separaci√≥n de clusters y supera los umbrales de calidad definidos.  

### üè¢ 37. Decisi√≥n para el Pipeline Final

En este paso se tom√≥ la **decisi√≥n final** sobre qu√© m√©todo utilizar en el **pipeline de clustering**, bas√°ndose en los resultados de desempe√±o obtenidos en los pasos anteriores.

#### üìà Resultados

El m√©todo finalmente seleccionado fue:  

* **üéØ SELECCIONADO:** `PCA (2D)`  
* **‚úÖ RAZ√ìN:** Mejor balance entre reducci√≥n dimensional y performance  
* **üìä Preparado para clustering:**  
  * M√©todo: `PCA`  
  * Dimensiones: `(200, 2)`  
  * Silhouette Score: `0.686`  

* La elecci√≥n de **PCA 2D** confirma que la reducci√≥n de dimensionalidad no solo simplifica el dataset, sino que adem√°s mejora la calidad de los clusters.  
* Forward y Backward Selection ofrecieron resultados aceptables, pero **no superaron a PCA en desempe√±o global**.  
* Baseline fue descartado por su bajo score (`0.364`), insuficiente para un clustering robusto.  

El **pipeline final de clustering se construir√° sobre PCA en 2D**, optimizando el balance entre simplicidad, desempe√±o y calidad de segmentaci√≥n.  

### ü§ñ 38. K-Means Clustering - Encontrando los Grupos (30 min)

Se aplic√≥ **K-Means Clustering** para descubrir **segmentos de clientes** en el dataset transformado mediante `PCA`.

**Dataset:** `(200, 2)`  
**M√©todo de reducci√≥n:** `PCA`

#### 1Ô∏è‚É£ B√∫squeda del K √ìptimo

Se evaluaron valores de **K entre 2 y 8** utilizando dos m√©tricas principales:

* **Inertia (WCSS):** mide la compacidad de los clusters.
* **Silhouette Score:** mide la separaci√≥n y cohesi√≥n de los clusters.

**Resultados obtenidos:**

| K | Inertia | Silhouette |
| - | ------- | ---------- |
| 2 | 18.62   | 0.762      |
| 3 | 10.93   | 0.742      |
| 4 | 3.78    | 0.686      |
| 5 | 2.78    | 0.656      |
| 6 | 1.89    | 0.619      |
| 7 | 1.43    | 0.607      |
| 8 | 1.14    | 0.597      |


üìä Se graficaron ambos m√©todos para identificar el K m√°s adecuado (ver en evidencias):

* **Elbow Method (izquierda):** permite observar d√≥nde la reducci√≥n de inercia deja de ser significativa.  
* **Silhouette Analysis (derecha):** mide la calidad de los clusters, siendo >0.7 muy bueno y >0.5 aceptable.


#### 3Ô∏è‚É£ Deep Dive - Elbow Method

üìâ **¬øQu√© es exactamente "el codo"?**

* **Matem√°ticamente:** punto donde la segunda derivada de WCSS cambia m√°s bruscamente.  
* **Visualmente:** transici√≥n de ca√≠da empinada a ca√≠da suave.  
* **Conceptualmente:** balance entre simplicidad y precisi√≥n.

**An√°lisis cuantitativo:**

* K=2: Œî Inertia = -7.68, Œî¬≤ = 0.53  
* K=3: Œî Inertia = -7.15, Œî¬≤ = 6.16  
* K=4: Œî Inertia = -1.00, Œî¬≤ = 0.11  
* K=5: Œî Inertia = -0.89, Œî¬≤ = 0.43  
* K=6: Œî Inertia = -0.46, Œî¬≤ = 0.17  

üìç **Candidato por Elbow:** K=6  
üìç **Candidato por Silhouette:** K=2 (score=0.762)


#### 4Ô∏è‚É£ Decisi√≥n Final de K

* Elbow sugiere **K=6**.  
* Silhouette sugiere **K=2**.  
* Contexto de negocio sugiere entre **3 y 5 segmentos**.  

‚öñÔ∏è **Decisi√≥n:**  
‚û°Ô∏è Se eligi√≥ **K=4**, por ser un buen balance entre ambos m√©todos y el criterio de negocio.

Se entren√≥ el modelo final con **K=4**:

* üìä **Silhouette Score:** `0.686`  
* üéØ **Clusters encontrados:** `4`  
* üìà **Inertia final:** `3.78`

### 6Ô∏è‚É£ Distribuci√≥n de Clientes por Cluster

| Cluster | N¬∫ Clientes | Porcentaje |
| ------- | ----------- | ---------- |
| 0       | 57          | 28.5%      |
| 1       | 47          | 23.5%      |
| 2       | 55          | 27.5%      |
| 3       | 41          | 20.5%      |

### ü§ñ 39. An√°lisis de Clusters y Perfiles (25 min)


Se gener√≥ un **reporte ejecutivo de segmentos de clientes** a partir del modelo final de clustering.

#### 1Ô∏è‚É£ Perfiles Detallados por Cluster

**CLUSTER 0** (57 clientes, 28.5%)  
**Perfil Demogr√°fico:**  
- Edad promedio: 28.4 a√±os  
- Distribuci√≥n g√©nero: {'Female': 57}  

**Perfil Financiero:**  
- Ingreso anual: $59.7k  
- Spending Score: 67.7/100  


**CLUSTER 1** (47 clientes, 23.5%)  
**Perfil Demogr√°fico:**  
- Edad promedio: 50.1 a√±os  
- Distribuci√≥n g√©nero: {'Male': 47}  

**Perfil Financiero:**  
- Ingreso anual: $62.2k  
- Spending Score: 29.6/100  


**CLUSTER 2** (55 clientes, 27.5%)  
**Perfil Demogr√°fico:**  
- Edad promedio: 48.1 a√±os  
- Distribuci√≥n g√©nero: {'Female': 55}  

**Perfil Financiero:**  
- Ingreso anual: $58.8k  
- Spending Score: 34.8/100  


**CLUSTER 3** (41 clientes, 20.5%)  
**Perfil Demogr√°fico:**  
- Edad promedio: 28.0 a√±os  
- Distribuci√≥n g√©nero: {'Male': 41}  

**Perfil Financiero:**  
- Ingreso anual: $62.3k  
- Spending Score: 70.2/100  


#### 2Ô∏è‚É£ Visualizaci√≥n de Resultados

Se generaron gr√°ficos para una visi√≥n clara de los clusters (ver en evidencias):

1. **Clusters Descubiertos (PCA 2D):**  
   Representaci√≥n en dos dimensiones con los centroides marcados.  

2. **Perfil Promedio por Cluster:**  
   Comparaci√≥n de edad, ingreso anual y spending score promedio por cluster.  

3. **Distribuci√≥n de Clientes por Cluster:**  
   Cantidad absoluta y porcentaje de clientes en cada segmento.  

### üìä 40. An√°lisis Silhouette Detallado

En este paso se realiz√≥ un **an√°lisis exhaustivo del √≠ndice de Silhouette** para evaluar la calidad de los clusters a nivel individual y grupal.

El objetivo fue identificar no solo el valor global del Silhouette Score, sino tambi√©n la **consistencia interna de cada cluster**, detectando posibles casos de clientes mal asignados.

#### ‚öôÔ∏è Decisiones tomadas

Se completaron los espacios en blanco con:  

- `from sklearn.metrics import silhouette_samples`  
  - Esta funci√≥n permite calcular el **Silhouette Score de cada muestra individual**, a diferencia de `silhouette_score` que solo entrega un valor global.  
  - La elecci√≥n se bas√≥ en la necesidad de analizar el comportamiento **intra-cluster**, evaluando no solo la media global sino tambi√©n valores m√≠nimos que indican posibles **clientes frontera** o **outliers**.  

De esta manera, se pudieron generar estad√≠sticas detalladas por cluster: promedio (`Œº`), valor m√≠nimo (`min`) y cantidad de muestras (`samples`).

#### üìà Resultados

üìä **An√°lisis Silhouette Detallado:**

- üéØ **Silhouette Score General:** `0.686`

Por cluster:

- **Cluster 0:** Œº=0.671, min=0.091, samples=57  
- **Cluster 1:** Œº=0.659, min=0.156, samples=47  
- **Cluster 2:** Œº=0.671, min=0.371, samples=55  
- **Cluster 3:** Œº=0.759, min=0.001, samples=41  

- El **Silhouette Score general (0.686)** confirma que los clusters son consistentes y bien separados.  
- **Cluster 3** presenta la mejor cohesi√≥n interna (Œº=0.759), aunque tiene un valor m√≠nimo muy bajo (`0.001`), lo que sugiere que algunos clientes est√°n en la frontera entre segmentos.  
- **Cluster 1** es el m√°s heterog√©neo, con un promedio menor (0.659) y valores m√≠nimos relativamente bajos.  
- Los valores m√≠nimos en todos los clusters permiten detectar **clientes borderline**, que podr√≠an migrar a otro cluster con ligeros cambios en los datos.  

Este an√°lisis aporta una visi√≥n m√°s granular de la calidad del clustering, y constituye un insumo clave para la **validaci√≥n del modelo** y la **definici√≥n de estrategias de negocio espec√≠ficas** en pasos posteriores.

### üö® 41. Identificaci√≥n de Outliers

En este paso se realiz√≥ la **detecci√≥n de posibles outliers** dentro de los clusters formados en el proceso de segmentaci√≥n.  
El criterio utilizado se basa en el **Silhouette Score individual** de cada observaci√≥n, considerando que valores **negativos** indican una mala asignaci√≥n al cluster.

### üè∑Ô∏è 42. An√°lisis de Perfiles de Cliente

En este paso se construy√≥ un **reporte ejecutivo de segmentaci√≥n**, donde se describen los **perfiles de clientes agrupados por cluster**.  
El an√°lisis se realiz√≥ utilizando las variables **reales** del dataset:  

- **Edad (Age)**  
- **Ingreso Anual (Annual Income k$)**  
- **Spending Score (1‚Äì100)**  
- **G√©nero (Genre)**  

El objetivo fue **entender las caracter√≠sticas demogr√°ficas y financieras** de cada segmento, con el fin de interpretar los resultados del clustering y facilitar la toma de decisiones estrat√©gicas.

#### üìà Resultados

Se identificaron **cuatro perfiles principales**:  

- üè∑Ô∏è **Cluster 0** (57 clientes, 28.5%)  
  - üë§ Edad promedio: 28.4 a√±os  
  - üë• G√©nero: 100% Female  
  - üíµ Ingreso anual: 59.7k  
  - üõçÔ∏è Spending Score: 67.7/100  

- üè∑Ô∏è **Cluster 1** (47 clientes, 23.5%)  
  - üë§ Edad promedio: 50.1 a√±os  
  - üë• G√©nero: 100% Male  
  - üíµ Ingreso anual: 62.2k  
  - üõçÔ∏è Spending Score: 29.6/100  

- üè∑Ô∏è **Cluster 2** (55 clientes, 27.5%)  
  - üë§ Edad promedio: 48.1 a√±os  
  - üë• G√©nero: 100% Female  
  - üíµ Ingreso anual: 58.8k  
  - üõçÔ∏è Spending Score: 34.8/100  

- üè∑Ô∏è **Cluster 3** (41 clientes, 20.5%)  
  - üë§ Edad promedio: 28.0 a√±os  
  - üë• G√©nero: 100% Male  
  - üíµ Ingreso anual: 62.3k  
  - üõçÔ∏è Spending Score: 70.2/100  

#### üîé An√°lisis

- Los **Clusters 0 y 3** corresponden a clientes **j√≥venes** (‚âà28 a√±os), diferenciados principalmente por g√©nero y por un **alto spending score** (>67), lo que los convierte en segmentos con **gran potencial de consumo**.  
- Los **Clusters 1 y 2** agrupan a clientes **mayores (‚âà48‚Äì50 a√±os)** con ingresos similares (~60k), pero con **spending scores bajos** (<35), lo que los clasifica como segmentos de **bajo consumo**.  
- La segmentaci√≥n refleja patrones claros de comportamiento asociados a la combinaci√≥n de **edad + g√©nero + nivel de gasto**, facilitando la aplicaci√≥n de estrategias de marketing personalizadas.  

### üïµÔ∏è 43. Reflexiones de Data Detective

En esta secci√≥n se responden preguntas clave sobre el proceso anal√≠tico siguiendo la metodolog√≠a **CRISP-DM**, con foco en la preparaci√≥n de datos, clustering y aplicaci√≥n pr√°ctica de los resultados.

#### üîç Metodolog√≠a CRISP-DM

- **Fase m√°s desafiante:** la **Data Preparation**, debido a la heterogeneidad en las escalas y la necesidad de seleccionar features relevantes sin perder informaci√≥n.  
- **Influencia del negocio:** se priorizaron variables como **Annual Income** y **Spending Score** por su alto valor estrat√©gico para segmentar clientes y definir pol√≠ticas de **pricing** y **promociones**.

#### üßπ Data Preparation

- **Mejor scaler:** `MinMaxScaler`, ya que gener√≥ un mejor **Silhouette Score con K=4**, logrando balancear las magnitudes sin distorsionar la estructura de los datos.  
- **M√°s efectivo:** **PCA (2D)**, que super√≥ en desempe√±o a los m√©todos de **Forward/Backward Selection**.  
- **Interpretabilidad vs performance:** se utiliz√≥ **PCA** para mejorar el modelado y la visualizaci√≥n, mientras que la **Feature Selection** permiti√≥ explicar de forma clara los perfiles de clientes.

#### üß© Clustering

- **Elbow vs Silhouette:** no se evalu√≥ formalmente el m√©todo Elbow; la elecci√≥n de **K=4** se bas√≥ en los resultados del **Silhouette Score**.  
- **Coincidencia con la intuici√≥n de negocio:** s√≠, los clusters reflejan perfiles esperables:  
  - Alto ingreso ‚Äì alto gasto  
  - Alto ingreso ‚Äì bajo gasto  
  - Bajo ingreso ‚Äì alto gasto  
  - Bajo ingreso ‚Äì bajo gasto  
- **Qu√© har√≠a diferente:** probar **K en el rango [3..6]**, validar estabilidad con **bootstrap**, experimentar con modelos m√°s flexibles como **GMM** y revisar si las variables dummies de g√©nero realmente aportan valor.

#### üíº Aplicaci√≥n Pr√°ctica

- **Presentaci√≥n empresarial:** mediante un **gr√°fico PCA en 2D** acompa√±ado de una **tabla ejecutiva de perfiles y acciones recomendadas por cluster**.  
- **Valor aportado:** segmentaci√≥n **accionable** para dise√±ar campa√±as de marketing, definir **ofertas personalizadas**, optimizar **layout de tiendas** y mejorar la **retenci√≥n de clientes**.  
- **Limitaciones:**  
  - Dataset peque√±o y con pocas variables.  
  - Supuestos restrictivos de **K-Means** (clusters esf√©ricos).  
  - Reducci√≥n de interpretabilidad al aplicar **PCA**.  

### üöÄ Challenge 1: Algoritmos de Clustering Alternativos

En este desaf√≠o se exploraron **m√©todos alternativos de clustering** para complementar los resultados obtenidos con **K-Means**. El objetivo fue validar la robustez de la segmentaci√≥n y comparar diferentes enfoques basados en densidad, modelos probabil√≠sticos y m√©todos jer√°rquicos/espectrales.

#### üß© A. DBSCAN - Density-Based Clustering

DBSCAN permite detectar clusters de forma **no esf√©rica** y es especialmente √∫til para descubrir **outliers y ruido**.

##### ‚öôÔ∏è Decisiones tomadas
- Se importaron `DBSCAN` y `NearestNeighbors` de **scikit-learn** para calcular el **k-distance graph**.  
- Se seleccion√≥ **min_samples=5** como valor est√°ndar recomendado.  
- El par√°metro **eps** se estim√≥ con el gr√°fico de k-distancias (üìä *Figura 1*), tomando el **percentil 90** de las distancias. Esto asegura un balance entre formar clusters significativos y no sobreincluir ruido.  

##### üìà Resultados
- **Clusters encontrados:** 7  
- **Puntos de ruido:** 8 (‚âà4.0%)  
- El gr√°fico evidenci√≥ un ‚Äúcodo‚Äù claro en el rango de **0.08‚Äì0.10**, lo cual valida la elecci√≥n de `eps`.  

üìå *Figura 1: K-distance Graph para selecci√≥n de eps (ver en evidencias)*  

#### üß© B. HDBSCAN - Hierarchical Density-Based Clustering

HDBSCAN extiende DBSCAN y genera una jerarqu√≠a de clusters con mayor estabilidad y menos sensibilidad a los hiperpar√°metros.

##### ‚öôÔ∏è Decisiones tomadas
- Se aplic√≥ `hdbscan.HDBSCAN` con:  
  - `min_cluster_size=10` (para evitar micro-clusters poco √∫tiles).  
  - `min_samples=5` como tolerancia a ruido.  
- Se utiliz√≥ la m√©trica `euclidean`.  

##### üìà Resultados
- **Clusters encontrados:** 5  
- **Persistencia promedio:** entre 0.19 y 0.36 (moderada).  
- El √°rbol condensado (*Figura 2*) permiti√≥ visualizar las divisiones m√°s estables en los datos.  

üìå *Figura 2: HDBSCAN Condensed Tree con clusters destacados (ver en evidencias)*  


#### üß© C. Gaussian Mixture Models (GMM)

GMM modela los datos como combinaci√≥n de distribuciones gaussianas, permitiendo **soft clustering** (probabilidad de pertenencia a m√∫ltiples clusters).

##### ‚öôÔ∏è Decisiones tomadas
- Se us√≥ `GaussianMixture` de scikit-learn.  
- Se evaluaron modelos con **2 a 7 componentes**.  
- Se compararon **AIC y BIC**, priorizando BIC por su penalizaci√≥n m√°s estricta.  

##### üìà Resultados
- **N√∫mero √≥ptimo de componentes (BIC):** 4  
- **Log-likelihood:** 3.307  
- El gr√°fico (*Figura 3*) mostr√≥ que el BIC alcanz√≥ su m√≠nimo en **4 componentes**, confirmando un modelo compacto y robusto.  

üìå *Figura 3: Curvas AIC vs BIC para selecci√≥n de n√∫mero de componentes (ver en evidencias)*  

#### üß© D. Spectral Clustering & Agglomerative Clustering

##### ‚öôÔ∏è Decisiones tomadas
- Se implement√≥ `SpectralClustering` con `affinity='rbf'`, ya que este kernel captura relaciones no lineales mejor que `nearest_neighbors`.  
- Para Agglomerative Clustering se eligi√≥ `linkage='ward'`, ideal para datos continuos y que busca minimizar varianza intra-cluster.  
- En ambos casos se trabaj√≥ con **optimal_k=4** (en l√≠nea con Silhouette y GMM).  

##### üìà Resultados
- **Spectral Clustering:** completado con 4 clusters.  
- **Agglomerative Clustering:** completado con 4 clusters.  
- Ambos confirmaron una estructura coherente de **4 segmentos principales**, consistente con los m√©todos anteriores.  

#### üîé An√°lisis Comparativo

- **DBSCAN/HDBSCAN:** √∫tiles para detectar **ruido y clusters arbitrarios**, aunque m√°s sensibles a par√°metros.  
- **GMM:** permite **soft clustering** y confirm√≥ la segmentaci√≥n en 4 grupos.  
- **Spectral y Agglomerative:** reforzaron la existencia de **4 clusters principales** como estructura estable.  

üëâ La convergencia de varios algoritmos hacia **4 segmentos clave** valida la segmentaci√≥n y aumenta la confianza en la robustez del an√°lisis.


---

## Reflexi√≥n
La presente pr√°ctica tuvo como objetivo principal segmentar clientes de un dataset real utilizando t√©cnicas de preprocesamiento, reducci√≥n de dimensionalidad y clustering, con un enfoque integral que incluy√≥ an√°lisis exploratorio, selecci√≥n de features y comparaci√≥n de distintos algoritmos. A lo largo de los pasos se aplic√≥ un pipeline completo, siguiendo principios de buenas pr√°cticas de an√°lisis de datos y metodolog√≠as como CRISP-DM.

---

## Evidencias

* [C√≥digo ejecutado por partes en Google Colab](https://colab.research.google.com/drive/1sLAXDN0Ir6vpMGVrMpNp2ZqbJfRfzugt?usp=sharing)

### Gr√°fica 1 - Distribuci√≥n de variables:
![Distribuci√≥n de Variables](image.png)

### Gr√°fica 2 - Relaciones entre variables:
![Relaciones Entre Variables](image1.png)

### Gr√°fica 3 - Matriz de correlaci√≥n:
![Matriz de Correlaci√≥n](image2.png)

### Gr√°fica 4 - Comparaci√≥n de scalers:
![Comparaci√≥n de scalers](image3.png)

### Gr√°fica 5 - Annual Income: Original vs Scalers:
![Annual Income: Original vs Scalers](image4.png)

### Gr√°fica 6 - Varianzas en componentes:
![Varianzas en componentes](image5.png)

### Gr√°fica 7 - Mall customers en espacio PCA 2D:
![Mall customers en espacio PCA 2D](image6.png)

### Gr√°fica 8 - Comparaci√≥n de m√©todos:
![Comparaci√≥n de m√©todos](image7.png)

### Gr√°fica 9 - Visualizaci√≥n de m√©todos:
![Visualizaci√≥n de m√©todos](image8.png)

### Gr√°fica 10 - Visualizaci√≥n de clusters:
![Visualizaci√≥n de clusters](image9.png)

### Gr√°fica 11 - K-distance Graph para selecci√≥n de eps:
![K-distance Graph para selecci√≥n de eps](image10.png)

### Gr√°fica 12 - HDBSCAN Condensed Tree con clusters destacados:
![HDBSCAN Condensed Tree con clusters destacados](image11.png)

### Gr√°fica 13 - Curvas AIC vs BIC para selecci√≥n de n√∫mero de componentes:
![Curvas AIC vs BIC para selecci√≥n de n√∫mero de componentes](image12.png)


