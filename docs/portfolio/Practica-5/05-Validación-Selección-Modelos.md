
# üìù Pr√°ctica 5 ‚Äî Validaci√≥n y Selecci√≥n de Modelos 

## Contexto

En esta pr√°ctica trabajamos con un dataset de estudiantes con el objetivo de **predecir abandono, inscripci√≥n y graduaci√≥n** en educaci√≥n superior.  

El prop√≥sito fue **aprender t√©cnicas de validaci√≥n cruzada, comparar modelos de clasificaci√≥n, optimizar hiperpar√°metros y aplicar m√©todos de explicabilidad**.  

El enfoque combina aspectos **t√©cnicos** (uso de pipelines, cross-validation, GridSearchCV, RandomizedSearchCV, Random Forest) con aspectos **pr√°cticos** (interpretaci√≥n de resultados, detecci√≥n de sesgos y aplicabilidad en educaci√≥n).  

De esta forma, se busca no solo entrenar un modelo con buen desempe√±o, sino tambi√©n que sea confiable y explicable en un contexto real.

---

## üìã Marco Te√≥rico

En esta pr√°ctica se trabajan distintos conceptos fundamentales del *Machine Learning cl√°sico*, necesarios para comprender y aplicar t√©cnicas de validaci√≥n y selecci√≥n de modelos.

### Train/Test Split
Consiste en dividir los datos en dos subconjuntos:  
- **Entrenamiento**: utilizado para ajustar el modelo.  
- **Prueba**: utilizado para evaluar su rendimiento en datos no vistos.  

Este proceso permite estimar c√≥mo se comportar√° el modelo en producci√≥n y evita la sobreestimaci√≥n de su precisi√≥n.

### Clasificaci√≥n vs Regresi√≥n
- **Clasificaci√≥n**: la variable objetivo es categ√≥rica (ejemplo: abandono, inscrito, graduado).  
- **Regresi√≥n**: la variable objetivo es continua (ejemplo: predicci√≥n de ingresos, precio de una vivienda).  

La diferencia radica en el tipo de salida que produce cada modelo y en las m√©tricas usadas para evaluarlos.

### Data Leakage
El *data leakage* (contaminaci√≥n de datos) ocurre cuando informaci√≥n del conjunto de prueba o de datos futuros se filtra en el proceso de entrenamiento. Esto genera m√©tricas artificialmente infladas y modelos que fallan en producci√≥n.  
Un ejemplo com√∫n es normalizar o escalar todo el dataset antes de dividirlo en train/test.

### Pipeline
Un *pipeline* es una secuencia ordenada de pasos (transformaciones y modelo) que se ejecutan de forma autom√°tica. Los pipelines garantizan reproducibilidad, evitan errores de orden y ayudan a prevenir *data leakage*.  
Ejemplo: primero estandarizar los datos con `StandardScaler` y luego entrenar un modelo.

### Validaci√≥n Cruzada (Cross-Validation)
Es una t√©cnica que divide los datos en *k* particiones (folds). El modelo se entrena y valida *k* veces, usando en cada iteraci√≥n un fold diferente como prueba.  
El resultado final es el promedio de todas las pruebas, lo que ofrece una estimaci√≥n m√°s robusta y confiable del desempe√±o del modelo.

### M√©tricas de Estabilidad
Adem√°s del promedio de m√©tricas como accuracy, se analiza la desviaci√≥n est√°ndar obtenida en la validaci√≥n cruzada.  
- Un modelo **estable** tiene poca variaci√≥n entre folds.  
- Un modelo **inestable** puede mostrar buen desempe√±o en algunos folds y muy malo en otros.

### Comparaci√≥n de Modelos
Para seleccionar el mejor modelo se entrenan y eval√∫an diferentes algoritmos bajo las mismas condiciones.  
En este caso, se comparan modelos como **Logistic Regression, RidgeClassifier y Random Forest**, considerando no solo la precisi√≥n, sino tambi√©n su estabilidad y robustez.

---

## üéØ Objetivos

- Aprender a prevenir data leakage usando pipelines
- Implementar validaci√≥n cruzada (cross-validation) robusta
- Comparar m√∫ltiples modelos de forma sistem√°tica
- Interpretar m√©tricas de estabilidad y selecci√≥n de modelos

---

## Actividades (con tiempos estimados)

| Actividad                              | Resultado esperado                                                         |
| -------------------------------------- | -------------------------------------------------------------------------- |
| Setup inicial                          | Librer√≠as cargadas y entorno listo.                                        |
| Cargar y explorar datos de estudiantes | Dataset cargado y explorado.                                               |
| Preparar datos para validaci√≥n         | Variables listas en formato num√©rico y sin errores.                        |
| Implementar Validaci√≥n Cruzada         | Comparaci√≥n entre KFold y StratifiedKFold, an√°lisis de estabilidad.        |
| Competencia de M√∫ltiples Modelos       | Selecci√≥n del mejor algoritmo (Random Forest).                             |
| GridSearchCV y RandomizedSearchCV      | Optimizaci√≥n de hiperpar√°metros del modelo ganador.                        |
| Explicabilidad del modelo              | An√°lisis de caracter√≠sticas m√°s influyentes y decisiones individuales.     |
| Preguntas de Reflexi√≥n                 | Respuestas a cuestiones conceptuales clave sobre validaci√≥n y estabilidad. |

---

## Desarrollo

### üè† 1. Setup inicial

En este paso se prepara el entorno de trabajo para realizar la pr√°ctica. Esto incluye:
- Instalar librer√≠as necesarias como ucimlrepo para cargar datasets de UCI ML Repository.
- Importar librer√≠as esenciales para manipulaci√≥n de datos (pandas, numpy), visualizaci√≥n (matplotlib.pyplot) y machine learning (sklearn).
- Importar clases y funciones para modelado, validaci√≥n, escalado y pipelines.

Preparar el entorno asegura que todos los pasos posteriores se puedan ejecutar sin errores y evita problemas de dependencias o imports faltantes. Adem√°s, facilita la prevenci√≥n de data leakage, la validaci√≥n cruzada y el uso de pipelines, fundamentales para esta pr√°ctica.

#### üí° PISTAS:

- **RidgeClassifier** ‚Üí Regularizaci√≥n L2 para controlar complejidad del modelo lineal.
- **RandomForestClassifier** ‚Üí Combina varios √°rboles de decisi√≥n para mejorar precisi√≥n y estabilidad.
- **cross_val_score** ‚Üí Eval√∫a el modelo con validaci√≥n cruzada y evita usar los mismos datos para entrenar y validar.
- **StratifiedKFold** ‚Üí Mantiene proporci√≥n de clases en cada pliegue, importante en datasets desbalanceados.
- **StandardScaler** ‚Üí Normaliza caracter√≠sticas para que tengan la misma escala, importante para algoritmos basados en distancia o gradiente.

### üéì  2. Cargar y explorar datos de estudiantes

**Contexto de negocio:**

- Problema: Predecir abandono estudiantil y √©xito acad√©mico en educaci√≥n superior
- Objetivo: Identificar estudiantes en riesgo para implementar estrategias de apoyo
- Variables: 36 caracter√≠sticas (demogr√°ficas, acad√©micas, socioecon√≥micas)
- Valor: Reducir tasas de abandono, mejorar retenci√≥n estudiantil

- Se carga el dataset desde UCI ML Repository usando fetch_ucirepo.
- Se separan las features (X) de la variable objetivo (y).
- Se explora la informaci√≥n b√°sica del dataset: n√∫mero de estudiantes y caracter√≠sticas, nombres de las primeras columnas y estad√≠sticas de inter√©s (edad al matricularse, etc.).
- Se analiza la variable objetivo: clases presentes, distribuci√≥n y significado de cada categor√≠a.

Conocer el dataset permite entender la estructura de los datos antes de entrenar modelos.

Identificar si las clases est√°n balanceadas es clave para seleccionar m√©tricas adecuadas y t√©cnicas de validaci√≥n cruzada.

Revisar estad√≠sticas b√°sicas ayuda a detectar posibles valores at√≠picos o inconsistencias.


- **Muestras y caracter√≠sticas**  
  El dataset contiene **4424 estudiantes** y **36 caracter√≠sticas**.  
  Estas variables incluyen informaci√≥n de tipo **demogr√°fica** (edad, estado civil, nacionalidad), **acad√©mica** (curso, modalidad de asistencia, calificaciones previas) y **socioecon√≥mica** (nivel educativo de los padres, situaci√≥n laboral, entre otras).

- **Variable objetivo (`Target`)**  
  La variable de salida representa el resultado acad√©mico de cada estudiante. Existen **tres categor√≠as posibles**:  
  1. **Dropout** ‚Üí El estudiante abandon√≥ la carrera.  
  2. **Enrolled** ‚Üí El estudiante sigue inscrito pero no ha finalizado.  
  3. **Graduate** ‚Üí El estudiante complet√≥ la carrera y se gradu√≥.

- **Balance de clases**  
  Las clases **no est√°n balanceadas**: algunas categor√≠as (como abandono y graduaci√≥n) son m√°s frecuentes que otras (inscripci√≥n en curso).  
  Esto implica que la m√©trica de **accuracy** puede no ser suficiente y ser√° necesario usar m√©tricas como **F1-score** o **balanced accuracy** junto con validaci√≥n estratificada.

- **Ejemplo de variables**  
  Entre las primeras columnas del dataset se encuentran:  
  - *Marital Status* (estado civil)  
  - *Application mode* (modo de aplicaci√≥n)  
  - *Daytime/evening attendance* (asistencia diurna/nocturna)  
  - *Previous qualification (grade)* (nota de estudios previos)  
  - *Mother‚Äôs qualification* y *Father‚Äôs qualification* (educaci√≥n de los padres)  

- **Edad de matr√≠cula**  
  La variable **Age at enrollment** muestra:  
  - Promedio: **23.3 a√±os**  
  - Rango: **17 a 70 a√±os**

üìå **Conclusi√≥n de la exploraci√≥n**  
El dataset combina informaci√≥n de diferentes dominios (demogr√°fico, acad√©mico y socioecon√≥mico). La variable objetivo es **multiclase** con clases desbalanceadas, lo cual representa un desaf√≠o de modelado.  
Ser√° importante aplicar t√©cnicas de validaci√≥n adecuadas, como **StratifiedKFold**, para preservar la proporci√≥n de clases en los experimentos.

### üîß 3. Preparar datos para validaci√≥n

En este paso se adaptan las variables del dataset para que sean compatibles con los modelos de *scikit-learn*.  
La preparaci√≥n de datos es esencial porque muchas veces los datasets contienen variables en formato **string** o categ√≥rico que no pueden procesarse directamente en los algoritmos de machine learning.

Las acciones realizadas fueron:

- **Conversi√≥n de la variable objetivo (`Target`)**  
  El dataset trae la columna objetivo como categor√≠as (`Dropout`, `Enrolled`, `Graduate`).  
  Se cre√≥ un **mapeo num√©rico** para traducir estas clases a enteros:
  - `Dropout ‚Üí 0`
  - `Enrolled ‚Üí 1`
  - `Graduate ‚Üí 2`  

  Esto permite que *sklearn* entrene modelos de clasificaci√≥n correctamente.

- **Chequeo de tipos de datos**  
  Se verific√≥ si la variable objetivo estaba en formato string y, en caso afirmativo, se transform√≥ usando el diccionario `reverse_mapping`.

- **Separaci√≥n de features y target**  
  - `X_features`: contiene las 36 caracter√≠sticas del estudiante (variables demogr√°ficas, acad√©micas y socioecon√≥micas).  
  - `y_target`: contiene la variable de salida en formato num√©rico.  

- **Validaci√≥n de las dimensiones**  
  Se imprimieron las formas de `X` e `y`, adem√°s de las clases √∫nicas disponibles, para confirmar que los datos quedaron consistentes.


### üîÑ 4. Implementar Validaci√≥n Cruzada

En este paso buscamos responder la pregunta:  
**¬øQu√© tan estable es el desempe√±o de nuestro modelo seg√∫n la forma de partir los datos?**

#### ‚öôÔ∏è Decisiones tomadas

- **Pipeline con `StandardScaler` + `LogisticRegression`**  
  Se eligi√≥ este pipeline porque:
  - La regresi√≥n log√≠stica requiere que los datos est√©n **escalados** para evitar que variables con distinta magnitud dominen la funci√≥n de costo.  
  - El `StandardScaler` convierte todas las variables a media 0 y varianza 1, lo que mejora la convergencia del modelo.  
  - La regresi√≥n log√≠stica (`LogisticRegression`) es un modelo interpretable y adecuado para clasificaci√≥n multiclase (Dropout, Enrolled, Graduate).

- **Comparaci√≥n de dos t√©cnicas de validaci√≥n cruzada**:  
  - **KFold**: divide los datos en 5 particiones aleatorias, sin preocuparse por la proporci√≥n de clases.  
  - **StratifiedKFold**: divide en 5 particiones, pero manteniendo la **misma proporci√≥n de clases** en cada fold, algo fundamental en datasets desbalanceados.  

#### üìä Resultados obtenidos

- **KFold**  
  - Scores: `[0.7525, 0.7661, 0.7684, 0.7774, 0.7805]`  
  - Media: **0.7690**  
  - Desviaci√≥n est√°ndar: **0.0098**

- **StratifiedKFold**  
  - Scores: `[0.7684, 0.7684, 0.7627, 0.7548, 0.7545]`  
  - Media: **0.7618**  
  - Desviaci√≥n est√°ndar: **0.0061**

Se gener√≥ un gr√°fico de cajas comparando ambas t√©cnicas de validaci√≥n cruzada que se encuentra en evidencias como grafica 1.

- El **KFold** mostr√≥ una media m√°s alta de accuracy (~0.769), pero con **mayor variabilidad** en los resultados.  
- El **StratifiedKFold** tuvo una media ligeramente inferior (~0.762), pero con **menor dispersi√≥n**, lo que indica **mayor estabilidad**.  

Entonces:

- **KFold**: logra mejor media de accuracy, pero es menos estable.  
- **StratifiedKFold**: aunque el promedio es un poco m√°s bajo, garantiza que cada fold represente bien las proporciones de clases.  

### üèÜ 5. Competencia de M√∫ltiples Modelos

En este paso realizamos una **competencia entre diferentes algoritmos de clasificaci√≥n** para identificar cu√°l ofrece mejor desempe√±o en el diagn√≥stico m√©dico.  

#### ‚öôÔ∏è Decisiones tomadas

1. **Logistic Regression (con escalado)**  
   - Se utiliz√≥ un `Pipeline` con `StandardScaler` y `LogisticRegression`.  
   - La regresi√≥n log√≠stica requiere escalado para que la magnitud de las variables no afecte el ajuste del modelo.  
   - Es un modelo **interpretable** y ampliamente usado en problemas m√©dicos.

2. **Ridge Classifier (con regularizaci√≥n L2)**  
   - Variante de la regresi√≥n log√≠stica que incluye regularizaci√≥n L2 para evitar **overfitting**.  
   - Tambi√©n requiere escalado, por lo que se incluy√≥ `StandardScaler`.  

3. **Random Forest (ensemble, sin escalado)**  
   - Modelo basado en m√∫ltiples √°rboles de decisi√≥n (bagging).  
   - No requiere escalado ya que los √°rboles se basan en umbrales de variables y no en magnitudes.  
   - Se espera un mejor desempe√±o en datasets con relaciones no lineales.  

#### üìä Resultados obtenidos

- **Logistic Regression**  
  - Accuracy promedio: **0.7618**  
  - Desviaci√≥n est√°ndar: **0.0061**  
  - Muy estable (scores: `[0.768, 0.768, 0.763, 0.755, 0.755]`)

- **Ridge Classifier**  
  - Accuracy promedio: **0.7509**  
  - Desviaci√≥n est√°ndar: **0.0032**  
  - Muy estable, pero con menor desempe√±o que la regresi√≥n log√≠stica est√°ndar.  

- **Random Forest**  
  - Accuracy promedio: **0.7658**  
  - Desviaci√≥n est√°ndar: **0.0064**  
  - Muy estable y el **mejor resultado global**.  

Se generaron dos gr√°ficas comparativas que se encuentran en evidencias como grafica 2.

1. **Distribuci√≥n de Accuracy por Modelo (Boxplot)**  
   - Permite ver la dispersi√≥n de los scores de validaci√≥n cruzada.  
   - El modelo **Random Forest** muestra valores consistentemente m√°s altos.  

2. **Accuracy Promedio ¬± Desviaci√≥n Est√°ndar (Barplot)**  
   - Resume el rendimiento promedio de cada modelo junto con su estabilidad.  
   - El **Random Forest** lidera en accuracy, mientras que Ridge queda rezagado.  

- **Ganador**: **Random Forest**, con un accuracy promedio de **0.7658** y variabilidad baja.  
- **Ridge Classifier** result√≥ el m√°s d√©bil, lo que muestra que en este dataset la regularizaci√≥n fuerte no fue beneficiosa.  
- **Logistic Regression** se mantiene como alternativa estable e interpretable, aunque con menor rendimiento que Random Forest.  

### üéÅ BONUS: ¬øQu√© significan las m√©tricas de validaci√≥n?

- **Cross-Validation**:  
  T√©cnica que divide los datos en **k partes (folds)** para entrenar y evaluar m√∫ltiples veces.  
  Permite estimar el desempe√±o del modelo de forma m√°s robusta que un √∫nico split.

- **Accuracy promedio**:  
  La **media** de rendimiento esperado en datos nuevos.  
  Resume el desempe√±o global del modelo en todos los folds.

- **Desviaci√≥n est√°ndar**:  
  Indica qu√© tan **estable o variable** es el modelo entre diferentes divisiones de datos.  
  Valores bajos significan mayor consistencia.

- **StratifiedKFold**:  
  Mantiene la **proporci√≥n** de clases en cada fold, lo cual es especialmente importante en datasets desbalanceados para evitar sesgos.

### ‚öôÔ∏è 6. GridSearchCV y RandomizedSearchCV

En este paso realizamos la **optimizaci√≥n de hiperpar√°metros** para el modelo ganador de la competencia (**Random Forest**).  
La motivaci√≥n es que, incluso despu√©s de elegir el mejor algoritmo, su rendimiento puede variar seg√∫n los valores de sus hiperpar√°metros.  
Por lo tanto, necesitamos ajustar dichos par√°metros de forma sistem√°tica.

#### üîë Decisiones tomadas

1. **Selecci√≥n de m√©todos de b√∫squeda**
   - Usamos **GridSearchCV** y **RandomizedSearchCV** de `sklearn.model_selection`.  
   - Ambos permiten explorar combinaciones de hiperpar√°metros, pero con estrategias diferentes:  
     - **GridSearchCV** prueba **todas las combinaciones posibles** (exhaustivo).  
     - **RandomizedSearchCV** selecciona **combinaciones aleatorias** (m√°s r√°pido en espacios grandes).  

2. **Espacios de b√∫squeda**
   - **Random Forest**: n√∫mero de estimadores, profundidad m√°xima y tama√±o m√≠nimo de split.  
   - **Logistic Regression**: par√°metro de regularizaci√≥n `C` y n√∫mero de iteraciones.  
   - **Ridge**: fuerza de regularizaci√≥n `alpha`.  

   Como el modelo ganador fue **Random Forest**, el espacio de b√∫squeda usado incluy√≥:
   - `n_estimators`: [50, 100, 200]  
   - `max_depth`: [None, 10, 20, 30]  
   - `min_samples_split`: [2, 5, 10]  

#### üìä Resultados obtenidos

- **GridSearchCV** (36 combinaciones probadas):  
  - Mejores par√°metros: `max_depth=None`, `min_samples_split=5`, `n_estimators=100`  
  - Accuracy promedio: **0.7783**  

- **RandomizedSearchCV** (20 combinaciones aleatorias):  
  - Mejores par√°metros: `n_estimators=100`, `min_samples_split=5`, `max_depth=30`  
  - Accuracy promedio: **0.7783**  

**Comparaci√≥n de eficiencia:**
- GridSearch evalu√≥ **36 configuraciones** ‚Üí m√°s exhaustivo pero m√°s lento.  
- RandomizedSearch evalu√≥ **20 configuraciones** ‚Üí m√°s r√°pido, mismo resultado.  

**Modelo final optimizado:**  
- Accuracy promedio = **0.7783 ¬± 0.0067**  

#### üßæ Interpretaci√≥n

- Ambos m√©todos encontraron configuraciones con desempe√±o muy similar.  
- **GridSearchCV** asegura explorar todo el espacio definido, pero es m√°s costoso computacionalmente.  
- **RandomizedSearchCV** fue m√°s eficiente, obteniendo pr√°cticamente el mismo resultado en menos tiempo.  
- El **Random Forest optimizado** mantiene un rendimiento estable y es el modelo final seleccionado.  

#### üí° Gu√≠a de decisi√≥n (cu√°ndo usar cada m√©todo)

- **GridSearchCV** cuando tienes **pocos** hiperpar√°metros y **suficiente** tiempo de c√≥mputo.  
- **RandomizedSearchCV** cuando tienes **muchos** hiperpar√°metros o tiempo limitado.  
- **Pipeline + SearchCV** siempre previene **data leakage** autom√°ticamente.  
- **cross_val_score** en el resultado final valida que la optimizaci√≥n no caus√≥ **overfitting**.  

### üîç 7. ¬øPor qu√© el modelo toma esas decisiones?

En este paso buscamos **explicabilidad del modelo ganador (Random Forest)** para comprender c√≥mo llega a sus predicciones y qu√© factores influyen en el abandono o √©xito estudiantil.  
La motivaci√≥n es que un modelo no solo debe ser preciso, sino tambi√©n **interpretable**, para que las decisiones puedan respaldar intervenciones reales en la educaci√≥n superior.

#### üîë Decisiones tomadas

1. **Modelo para explicabilidad**
   - Se utiliz√≥ el **Random Forest optimizado** (modelo ganador).  
   - Al no requerir escalado, se trabaj√≥ directamente con los datos originales.  

2. **T√©cnicas aplicadas**
   - **Feature Importance** ‚Üí Importancia relativa de cada caracter√≠stica.  
   - **An√°lisis por categor√≠as** ‚Üí Se agruparon variables en factores acad√©micos, demogr√°ficos y econ√≥micos.  
   - **Predicciones individuales** ‚Üí Ejemplo de interpretaci√≥n para un estudiante en riesgo.  
   - **Visualizaci√≥n de √°rboles** ‚Üí Se graficaron √°rboles individuales para mostrar reglas de decisi√≥n.  
   - **Diversidad del bosque** ‚Üí Se explic√≥ c√≥mo la variabilidad entre √°rboles potencia al Random Forest.  

#### üìä Resultados obtenidos

- **Top 10 caracter√≠sticas m√°s importantes:**
  1. Curricular units 2nd sem (approved) ‚Üí **0.1516**
  2. Curricular units 2nd sem (grade) ‚Üí **0.1193**
  3. Curricular units 1st sem (approved) ‚Üí **0.0987**
  4. Curricular units 1st sem (grade) ‚Üí **0.0589**
  5. Tuition fees up to date ‚Üí **0.0466**
  6. Curricular units 2nd sem (evaluations) ‚Üí **0.0419**
  7. Admission grade ‚Üí **0.0385**
  8. Age at enrollment ‚Üí **0.0372**
  9. Curricular units 1st sem (evaluations) ‚Üí **0.0349**
  10. Previous qualification (grade) ‚Üí **0.0343**

- Se gener√≥ la **Gr√°fica 3** mostrando las 15 caracter√≠sticas m√°s importantes se encuentra en evidencias.
- La caracter√≠stica m√°s relevante fue: **Curricular units 2nd sem (approved)**.  
- Esto sugiere que para reducir el abandono estudiantil se debe intervenir en:
  1. Monitorear y mejorar: Curricular units 2nd sem (approved)  
  2. Monitorear y mejorar: Curricular units 2nd sem (grade)  
  3. Monitorear y mejorar: Curricular units 1st sem (approved)  

**üë§ An√°lisis individual**

- Estudiante #0 ‚Üí Predicci√≥n: **Dropout**  
  - Probabilidades:  
    - Dropout: **73.7%**  
    - Enrolled: 8.2%  
    - Graduate: 18.1%  

- **Top 5 caracter√≠sticas que influyeron en esta predicci√≥n:**
  - Curricular units 2nd sem (approved) ‚Üí 0.00 (importancia 0.1516)  
  - Curricular units 2nd sem (grade) ‚Üí 0.00 (importancia 0.1193)  
  - Curricular units 1st sem (approved) ‚Üí 0.00 (importancia 0.0987)  
  - Curricular units 1st sem (grade) ‚Üí 0.00 (importancia 0.0589)  
  - Tuition fees up to date ‚Üí 1.00 (importancia 0.0466)  

- Esto muestra c√≥mo el **desempe√±o en los primeros semestres** y el **pago de matr√≠cula** son determinantes en el riesgo de abandono.

**Visualizaci√≥n de √°rboles**
- Se graficaron **3 √°rboles representativos de los 100 del bosque**.  
- Promedio de profundidad (5 primeros √°rboles): **21.2**  
- Promedio de nodos: **1139**  

La **Gr√°fica 4** en evidencias muestra ejemplos de reglas de decisi√≥n de los √°rboles con profundidad limitada (m√°x=3).  

- Ejemplo de regla:  
|--- Curricular units 2nd sem (approved) <= 3.50

| |--- Curricular units 2nd sem (evaluations) <= 7.50

| | |--- Curricular units 1st sem (enrolled) <= 0.50

 **üå≤ Diversidad del bosque**

El poder del **Random Forest** proviene de la **diversidad entre sus √°rboles**:
- Cada √°rbol se entrena con una muestra distinta (**bootstrap**).  
- Cada divisi√≥n de nodos usa un subconjunto aleatorio de caracter√≠sticas.  
- La predicci√≥n final es el **voto mayoritario**.  

Ejemplo (Estudiante #0):  
- √Årbol 1 ‚Üí Graduate  
- √Årbol 2 ‚Üí Dropout  
- √Årbol 3 ‚Üí Dropout  
- √Årbol 4 ‚Üí Dropout  
- √Årbol 5 ‚Üí Dropout  
- **Predicci√≥n final** ‚Üí Dropout (voto mayoritario).  

- Los **factores acad√©micos** explican la mayor√≠a de las decisiones del modelo.  
- La explicabilidad permite generar **acciones concretas de intervenci√≥n** en estudiantes en riesgo.  
- El an√°lisis individual muestra c√≥mo los **primeros semestres son cr√≠ticos** para predecir abandono.  
- Los √°rboles individuales ilustran las **reglas de decisi√≥n** que sustentan las predicciones globales.  

#### ¬øPor qu√© es importante la explicabilidad?

- **Confianza:** Los educadores necesitan **entender** por qu√© el modelo predice abandono.  
- **Intervenciones:** Knowing las caracter√≠sticas importantes permite crear **estrategias** espec√≠ficas.  
- **Bias detection:** La explicabilidad ayuda a detectar **sesgos** en el modelo.  
- **Regulaciones:** Muchos contextos requieren modelos **interpretables** por ley.  
- **Mejora continua:** Entender el modelo ayuda a **optimizar** futuras versiones.  

### üèÅ 8. Preguntas de Reflexi√≥n

#### ‚ùì ¬øQu√© es *data leakage* y por qu√© es peligroso?
El *data leakage* ocurre cuando el modelo tiene acceso a informaci√≥n del conjunto de prueba (o del futuro) durante el entrenamiento.  

Es peligroso porque el modelo aprende con datos que no deber√≠a ver, generando m√©tricas artificialmente altas pero un mal desempe√±o real en producci√≥n.

#### ‚ùì ¬øCu√°ndo usar *KFold* vs *StratifiedKFold*?
- **KFold:** se usa cuando las clases est√°n balanceadas o no importa mantener la proporci√≥n de clases en cada pliegue.  
- **StratifiedKFold:** se usa en clasificaci√≥n cuando las clases est√°n desbalanceadas, ya que asegura que cada pliegue mantenga la misma proporci√≥n de clases que el dataset completo.

#### ‚ùì ¬øC√≥mo interpretar "95.2% ¬± 2.1%" en *cross-validation*?
- **95.2%:** es el promedio de exactitud (accuracy) obtenido en las distintas particiones.  
- **¬± 2.1%:** es la desviaci√≥n est√°ndar, que mide la variabilidad entre pliegues. Un valor bajo indica que el modelo es estable y consistente.

#### ‚ùì ¬øPor qu√© *Random Forest* no necesita *StandardScaler*?
Porque los √°rboles de decisi√≥n (y, por extensi√≥n, los *Random Forests*) no se basan en distancias ni pendientes.  

Ellos dividen los datos en base a umbrales en las caracter√≠sticas, por lo que la escala de las variables no afecta la construcci√≥n de los √°rboles.

#### ‚ùì En diagn√≥stico m√©dico, ¬øprefieres un modelo con 98% accuracy pero inestable, o 95% accuracy pero muy estable?
Se prefiere el modelo con **95% de accuracy pero estable**, porque en contextos m√©dicos la confiabilidad y consistencia son m√°s importantes que un m√°ximo rendimiento puntual.  

Un modelo inestable puede dar resultados contradictorios y generar riesgos en diagn√≥sticos cr√≠ticos.

---

## Reflexi√≥n

Esta pr√°ctica permiti√≥ integrar varios conceptos fundamentales de **machine learning aplicado**:  
- Aprendimos la importancia de usar **pipelines y validaci√≥n cruzada** para evitar data leakage y obtener m√©tricas m√°s realistas.  
- Comprobamos que la elecci√≥n de la t√©cnica de validaci√≥n (KFold vs StratifiedKFold) influye directamente en la **estabilidad del modelo**.  
- La comparaci√≥n de modelos mostr√≥ que los algoritmos basados en ensambles como **Random Forest** pueden superar en rendimiento a modelos lineales, sin necesidad de escalado.  
- Con la optimizaci√≥n de hiperpar√°metros, entendimos que incluso un buen modelo puede mejorar al ajustar sus configuraciones.  
- Finalmente, la explicabilidad nos record√≥ que no basta con tener un modelo preciso: tambi√©n debe ser **comprensible, confiable y √©ticamente responsable** para poder aplicarse en contextos sensibles como la educaci√≥n o la salud.  

En resumen, la pr√°ctica no solo fortaleci√≥ habilidades t√©cnicas, sino tambi√©n el criterio para seleccionar, validar y explicar modelos en escenarios reales.

---

## Evidencias

* [C√≥digo ejecutado por partes en Google Colab](https://colab.research.google.com/drive/1jRoMeZl_ik6VE4CDl8ydQ0eVNZFfg-gB?usp=sharing)

### Gr√°fica 1:
![Distribuci√≥n de Scores](image.png)

### Gr√°fica 2:
![Distribuci√≥n de Accuracy](image1.png)

### Gr√°fica 3:
![Caracteristicas](image2.png)

### Gr√°fica 2:
![√°rboles](image3.png)


### C√≥digo completo que se ejecut√≥:

```python
!pip install ucimlrepo
# Importar librer√≠as que vamos a usar
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Para validaci√≥n y selecci√≥n de modelos
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
# Para cargar datos desde UCI ML Repository
from ucimlrepo import fetch_ucirepo
from sklearn.metrics import accuracy_score, classification_report

print("Setup completo!")

# Cargar dataset de estudiantes desde UCI
student_data = fetch_ucirepo(id=697)

# Preparar datos
X = student_data.data.features
y = student_data.data.targets

print("Dataset: Student Dropout and Academic Success")
print(f"Estudiantes: {X.shape[0]}, Caracter√≠sticas: {X.shape[1]}")
print(f"Objetivo: Predecir {len(y.columns)} variable(s)")

# Explorar variable objetivo
target_col = y.columns[0]  # Primera columna objetivo
y_series = y[target_col]
print(f"\nVariable objetivo: {target_col}")

# Mapear valores para mejor interpretaci√≥n
target_mapping = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}
y_mapped = y_series.map(target_mapping)

# Distribuci√≥n de clases
print("\nDistribuci√≥n de resultados acad√©micos:")
value_counts = y_mapped.value_counts()
for outcome, count in value_counts.items():
    percentage = (count / len(y_mapped)) * 100
    print(f"  {outcome}: {count} estudiantes ({percentage:.1f}%)")

# Ver algunas caracter√≠sticas
print(f"\nPrimeras caracter√≠sticas:")
print(X.columns.tolist()[:10], "...")

# Estad√≠sticas b√°sicas
print(f"\nAge at enrollment:")
if 'Age at enrollment' in X.columns:
    age_col = X['Age at enrollment']
    print(f"  Promedio: {age_col.mean():.1f} a√±os")
    print(f"  Rango: {age_col.min():.0f}-{age_col.max():.0f} a√±os")

# Preparar variable objetivo como serie simple
# Convertir strings a n√∫meros para sklearn
target_mapping = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}
reverse_mapping = {'Dropout': 0, 'Enrolled': 1, 'Graduate': 2}

# Si y_series contiene strings, convertir a n√∫meros
if y_series.dtype == 'object':
    y_target = y_series.map(reverse_mapping)
else:
    y_target = y_series

X_features = X       # Features del dataset

print("Datos preparados para validaci√≥n:")
print(f"X shape: {X_features.shape}")
print(f"y shape: {y_target.shape}")
print(f"Clases √∫nicas: {sorted(y_target.unique())}")
print(f"Mapeo: {target_mapping}")

# === VALIDACI√ìN CRUZADA PARA ESTABILIDAD ===

print("üî¨ VALIDACI√ìN CRUZADA: ¬øQu√© tan estable es nuestro modelo?")

# 1. Crear pipeline robusto para usar en CV
pipeline_robust = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(max_iter=1000, random_state=42))
])

print("Pipeline creado para validaci√≥n cruzada")

# 2. Crear KFold b√°sico
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# 3. Evaluar con KFold usando cross_val_score
scores_kfold = cross_val_score(
    pipeline_robust, X_features, y_target, cv=kfold, scoring='accuracy'
)

print(f"\nKFOLD RESULTS:")
print(f"   Scores individuales: {scores_kfold}")
print(f"   Media: {scores_kfold.mean():.4f}")
print(f"   Desviaci√≥n est√°ndar: {scores_kfold.std():.4f}")
print(f"   Resultado: {scores_kfold.mean():.4f} ¬± {scores_kfold.std():.4f}")

# 4. Crear StratifiedKFold (mantiene proporci√≥n de clases)
stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 5. Evaluar con StratifiedKFold
scores_stratified = cross_val_score(
    pipeline_robust, X_features, y_target, cv= stratified_kfold, scoring='accuracy'
)

print(f"\nSTRATIFIED KFOLD RESULTS:")
print(f"   Scores individuales: {scores_stratified}")
print(f"   Media: {scores_stratified.mean():.4f}")
print(f"   Desviaci√≥n est√°ndar: {scores_stratified.std():.4f}")
print(f"   Resultado: {scores_stratified.mean():.4f} ¬± {scores_stratified.std():.4f}")

# 6. Comparar estabilidad (menor desviaci√≥n = m√°s estable)
print(f"\nCOMPARACI√ìN DE ESTABILIDAD:")
if scores_stratified.std() < scores_kfold.std():
    print("   StratifiedKFold es M√ÅS ESTABLE (menor variabilidad)")
    mejor_cv = "StratifiedKFold"
else:
    print("   KFold es M√ÅS ESTABLE (menor variabilidad)")
    mejor_cv = "KFold"

print(f"   Recomendaci√≥n: Usar {mejor_cv} para este dataset")

# 7. Visualizar la distribuci√≥n de scores
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.boxplot([scores_kfold, scores_stratified], labels=['KFold', 'StratifiedKFold'])
plt.title('Distribuci√≥n de Scores - Validaci√≥n Cruzada')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)
plt.show()

# === COMPETENCIA DE MODELOS ===

print("üèÜ TORNEO: ¬øCu√°l modelo funciona mejor para diagn√≥stico m√©dico?")

# 1. Definir candidatos (diferentes algoritmos)
models = {
    'Logistic Regression': Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(max_iter=1000, random_state=42))
    ]),

    # 2. Ridge Classifier (regresi√≥n log√≠stica con regularizaci√≥n L2)
    'Ridge Classifier': Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', RidgeClassifier(alpha=1.0, random_state=42))
    ]),

    # 3. Random Forest (ensemble, no necesita escalado)
    'Random Forest': Pipeline([
        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
    ])
}

print(f"Modelos en competencia: {list(models.keys())}")

# 4. Evaluar cada modelo con validaci√≥n cruzada
print(f"\nEVALUANDO MODELOS CON 5-FOLD CV...")

results = {}
for name, model in models.items():
    print(f"   Evaluando {name}...")

    # Usar StratifiedKFold para mantener balance de clases
    scores = cross_val_score(
        model, X_features, y_target,
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
        scoring='accuracy'
    )

    results[name] = scores

    print(f"   {name}: {scores.mean():.4f} ¬± {scores.std():.4f}")
    print(f"      Scores: {[f'{s:.3f}' for s in scores]}")

# 5. Encontrar el mejor modelo
print(f"\nRESULTADOS FINALES:")

# Encontrar modelo con mayor accuracy promedio
best_mean_score = 0
best_model_name = ""

for name, scores in results.items():
    if scores.mean() > best_mean_score:
        best_mean_score = scores.mean()
        best_model_name = name

print(f"GANADOR: {best_model_name}")
print(f"Score: {best_mean_score:.4f}")

# 6. An√°lisis detallado de estabilidad
print(f"\nAN√ÅLISIS DE ESTABILIDAD:")
for name, scores in results.items():
    stability = scores.std()

    if stability < 0.02:
        status = "MUY ESTABLE"
    elif stability < 0.05:
        status = "ESTABLE"
    else:
        status = "INESTABLE"

    print(f"   {name}: {status} (std: {stability:.4f})")

# 7. Visualizaci√≥n comparativa
plt.figure(figsize=(12, 6))

# Boxplot de distribuci√≥n de scores
plt.subplot(1, 2, 1)
plt.boxplot([results[name] for name in models.keys()],
           labels=[name.split()[0] for name in models.keys()])
plt.title('Distribuci√≥n de Accuracy por Modelo')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)

# Barplot de medias con error bars
plt.subplot(1, 2, 2)
names = list(models.keys())
means = [results[name].mean() for name in names]
stds = [results[name].std() for name in names]

plt.bar(range(len(names)), means, yerr=stds, capsize=5)
plt.xticks(range(len(names)), [name.split()[0] for name in names])
plt.title('Accuracy Promedio ¬± Desviaci√≥n Est√°ndar')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Seleccionar el mejor modelo de la competencia anterior
best_model_base = models[best_model_name]

print(f"Optimizando hiperpar√°metros para: {best_model_name}")

# Definir espacio de b√∫squeda de hiperpar√°metros
if 'Random Forest' in best_model_name:
    param_grid = {
        'classifier__n_estimators': [50, 100, 200],
        'classifier__max_depth': [None, 10, 20, 30],
        'classifier__min_samples_split': [2, 5, 10]
    }
elif 'Logistic' in best_model_name:
    param_grid = {
        'classifier__C': [0.1, 1, 10, 100],
        'classifier__max_iter': [1000, 2000]
    }
else:  # Ridge
    param_grid = {
        'classifier__alpha': [0.1, 1, 10, 100]
    }

# M√âTODO 1: GridSearchCV (b√∫squeda exhaustiva)
print("\nM√©todo 1: GridSearchCV (b√∫squeda exhaustiva)")
grid_search = GridSearchCV(
    best_model_base,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_features, y_target)

print(f"Mejores par√°metros (Grid): {grid_search.best_params_}")
print(f"Mejor score (Grid): {grid_search.best_score_:.4f}")

# M√âTODO 2: RandomizedSearchCV (b√∫squeda aleatoria, m√°s eficiente)
print("\nM√©todo 2: RandomizedSearchCV (b√∫squeda aleatoria)")
random_search = RandomizedSearchCV(
    best_model_base,
    param_grid,
    n_iter=20,  # Solo 20 combinaciones aleatorias
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

random_search.fit(X_features, y_target)

print(f"Mejores par√°metros (Random): {random_search.best_params_}")
print(f"Mejor score (Random): {random_search.best_score_:.4f}")

# Comparar eficiencia
print(f"\nComparaci√≥n de eficiencia:")
print(f"GridSearch prob√≥: {len(grid_search.cv_results_['params'])} combinaciones")
print(f"RandomSearch prob√≥: {len(random_search.cv_results_['params'])} combinaciones")

# Evaluar modelo final optimizado
final_model = grid_search.best_estimator_
final_scores = cross_val_score(final_model, X_features, y_target, cv=5)
print(f"\nModelo final optimizado: {final_scores.mean():.4f} ¬± {final_scores.std():.4f}")

# Usar Random Forest para explicabilidad (si no gan√≥, crearlo)
if 'Random Forest' not in best_model_name:
    # Crear Random Forest espec√≠fico para explicabilidad
    # Random Forest no necesita escalado, as√≠ que lo omitimos para simplicidad
    rf_model = Pipeline([
        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
    ])
    rf_model.fit(X_features, y_target)
    print("Creado Random Forest espec√≠fico para an√°lisis de explicabilidad")
else:
    rf_model = final_model
    print("Usando el modelo ganador para explicabilidad")

# Verificar estructura del pipeline
print(f"Componentes del pipeline: {list(rf_model.named_steps.keys())}")

# 1. FEATURE IMPORTANCE - ¬øQu√© caracter√≠sticas son m√°s importantes?
feature_names = X_features.columns
importances = rf_model.named_steps['classifier'].feature_importances_

# Crear DataFrame para mejor visualizaci√≥n
feature_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)

print("\nTOP 10 CARACTER√çSTICAS M√ÅS IMPORTANTES:")
for i, row in feature_importance_df.head(10).iterrows():
    print(f"{row['feature']}: {row['importance']:.4f}")

# Visualizar importancia de caracter√≠sticas
plt.figure(figsize=(10, 8))
top_features = feature_importance_df.head(15)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Importancia')
plt.title('Top 15 Caracter√≠sticas M√°s Importantes para Predecir √âxito Estudiantil')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# 2. AN√ÅLISIS POR CATEGOR√çAS - Agrupar caracter√≠sticas relacionadas
academic_features = [col for col in feature_names if any(word in col.lower() 
                    for word in ['grade', 'units', 'curricular', 'semester'])]
demographic_features = [col for col in feature_names if any(word in col.lower() 
                       for word in ['age', 'gender', 'nationality', 'marital'])]
economic_features = [col for col in feature_names if any(word in col.lower() 
                    for word in ['scholarship', 'debt', 'fee', 'tuition'])]

def calculate_category_importance(features, importance_df):
    if not features:
        return 0
    category_importance = importance_df[importance_df['feature'].isin(features)]['importance'].sum()
    return category_importance

academic_importance = calculate_category_importance(academic_features, feature_importance_df)
demographic_importance = calculate_category_importance(demographic_features, feature_importance_df)
economic_importance = calculate_category_importance(economic_features, feature_importance_df)

print(f"\nIMPORTANCIA POR CATEGOR√çAS:")
print(f"Factores acad√©micos: {academic_importance:.4f}")
print(f"Factores demogr√°ficos: {demographic_importance:.4f}")
print(f"Factores econ√≥micos: {economic_importance:.4f}")

# 3. INTERPRETACI√ìN PR√ÅCTICA - ¬øQu√© significa esto?
print(f"\nINTERPRETACI√ìN PARA INTERVENCIONES:")
print(f"La caracter√≠stica m√°s importante es: {feature_importance_df.iloc[0]['feature']}")
print(f"Esto sugiere que para reducir abandono estudiantil debemos enfocarnos en:")

# Generar recomendaciones basadas en las top features
top_3_features = feature_importance_df.head(3)['feature'].tolist()
for i, feature in enumerate(top_3_features, 1):
    print(f"{i}. Monitorear y mejorar: {feature}")

# 4. PREDICCI√ìN INDIVIDUAL - ¬øPor qu√© un estudiante espec√≠fico est√° en riesgo?
print(f"\nAN√ÅLISIS DE ESTUDIANTE INDIVIDUAL (ejemplo):")
student_idx = 0
student_data = X_features.iloc[student_idx:student_idx+1]
prediction = rf_model.predict(student_data)[0]
prediction_proba = rf_model.predict_proba(student_data)[0]

# Definir mapeo localmente para esta secci√≥n
outcome_mapping = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}

# Manejar si prediction es string o n√∫mero
if isinstance(prediction, str):
    predicted_outcome = prediction
else:
    predicted_outcome = outcome_mapping[prediction]

print(f"Estudiante #{student_idx}:")
print(f"Predicci√≥n: {predicted_outcome}")
print(f"Probabilidades:")
for i, prob in enumerate(prediction_proba):
    outcome_name = outcome_mapping[i]
    print(f"  {outcome_name}: {prob:.3f}")

# Mostrar las caracter√≠sticas m√°s importantes de este estudiante
student_features = pd.DataFrame({
    'feature': feature_names,
    'value': student_data.iloc[0].values,
    'importance': importances
}).sort_values('importance', ascending=False)

print(f"\nTop 5 caracter√≠sticas que influyen en esta predicci√≥n:")
for i, row in student_features.head(5).iterrows():
    print(f"{row['feature']}: {row['value']:.2f} (importancia: {row['importance']:.4f})")

# 5. VISUALIZACI√ìN DE √ÅRBOLES INDIVIDUALES
print(f"\nVISUALIZACI√ìN DE √ÅRBOLES DEL RANDOM FOREST:")

# Instalar graphviz si no est√° disponible
try:
    from sklearn.tree import export_graphviz, plot_tree, export_text
    import matplotlib.pyplot as plt

    # Obtener algunos √°rboles del bosque
    forest = rf_model.named_steps['classifier']
    n_trees_to_show = min(3, len(forest.estimators_))

    print(f"Mostrando {n_trees_to_show} √°rboles de {len(forest.estimators_)} totales")

    # Visualizar √°rboles con plot_tree (m√°s simple)
    fig, axes = plt.subplots(1, n_trees_to_show, figsize=(25, 12))
    if n_trees_to_show == 1:
        axes = [axes]

    for i in range(n_trees_to_show):
        tree = forest.estimators_[i]

        # Limitar profundidad para que sea legible
        plot_tree(tree, 
                 ax=axes[i],
                 feature_names=list(feature_names),  # Usar todos los nombres de caracter√≠sticas
                 class_names=list(outcome_mapping.values()),
                 filled=True,
                 max_depth=3,  # Limitar profundidad
                 fontsize=6)  # Fuente m√°s peque√±a para que quepa

        axes[i].set_title(f'√Årbol {i+1} (profundidad m√°x: 3)', fontsize=12)

    plt.tight_layout()
    plt.show()

    # Informaci√≥n sobre la estructura de los √°rboles
    print(f"\nESTAD√çSTICAS DE LOS √ÅRBOLES:")
    depths = [tree.get_depth() for tree in forest.estimators_[:5]]
    n_nodes = [tree.tree_.node_count for tree in forest.estimators_[:5]]

    print(f"Profundidad promedio (primeros 5 √°rboles): {sum(depths)/len(depths):.1f}")
    print(f"N√∫mero promedio de nodos (primeros 5): {sum(n_nodes)/len(n_nodes):.1f}")

    # Mostrar un √°rbol muy simple por texto
    print(f"\nEJEMPLO DE REGLAS DE DECISI√ìN (√Årbol 1, simplificado):")
    tree_rules = export_text(forest.estimators_[0], 
                           feature_names=list(feature_names),
                           max_depth=2)
    print(tree_rules[:500] + "..." if len(tree_rules) > 500 else tree_rules)

except ImportError:
    print("Para visualizar √°rboles, instala: pip install graphviz")
    print("Alternativamente, mostramos la estructura del bosque:")

    forest = rf_model.named_steps['classifier']
    print(f"Random Forest contiene {len(forest.estimators_)} √°rboles")
    print(f"Cada √°rbol fue entrenado con {forest.max_features_} caracter√≠sticas aleatorias")

    # Estad√≠sticas b√°sicas sin visualizaci√≥n
    if len(forest.estimators_) > 0:
        depths = [tree.get_depth() for tree in forest.estimators_[:5]]
        print(f"Profundidad promedio: {sum(depths)/len(depths):.1f}")

# 6. DIVERSIDAD DEL BOSQUE
print(f"\nDIVERSIDAD EN EL RANDOM FOREST:")
print("El poder del Random Forest viene de la diversidad de sus √°rboles:")
print("- Cada √°rbol ve una muestra diferente de datos (bootstrap)")
print("- Cada split considera solo un subconjunto aleatorio de caracter√≠sticas")
print("- La predicci√≥n final es el voto mayoritario de todos los √°rboles")

# Mostrar diferencias en predicciones individuales
student_sample = X_features.iloc[0:1]
individual_predictions = []

# Preparar datos dependiendo de si el modelo tiene scaler o no
if 'scaler' in rf_model.named_steps:
    # Modelo con scaler
    scaled_sample = rf_model.named_steps['scaler'].transform(student_sample)
    print("Usando datos escalados para √°rboles individuales")
else:
    # Modelo sin scaler (ej: Random Forest sin preprocesamiento)
    scaled_sample = student_sample.values
    print("Usando datos sin escalar para √°rboles individuales")

for i, tree in enumerate(forest.estimators_[:5]):
    tree_pred = tree.predict(scaled_sample)[0]
    individual_predictions.append(tree_pred)

print(f"\nPredicciones de √°rboles individuales para el Estudiante #0:")
for i, pred in enumerate(individual_predictions):
    pred_name = outcome_mapping[pred] if isinstance(pred, int) else pred
    print(f"  √Årbol {i+1}: {pred_name}")

final_pred = max(set(individual_predictions), key=individual_predictions.count)
final_pred_name = outcome_mapping[final_pred] if isinstance(final_pred, int) else final_pred
print(f"Predicci√≥n final (voto mayoritario): {final_pred_name}")
```

---
## Referencias

- **UCI Machine Learning Repository** ‚Äî Student Performance Dataset  
- G√©ron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow*. O‚ÄôReilly.  
- Scikit-learn documentation: [https://scikit-learn.org/stable/](https://scikit-learn.org/stable/)  
- Molnar, C. (2022). *Interpretable Machine Learning*. [https://christophm.github.io/interpretable-ml-book/](https://christophm.github.io/interpretable-ml-book/)  
- Bishop, C. M. (2006). *Pattern Recognition and Machine Learning*. Springer.  